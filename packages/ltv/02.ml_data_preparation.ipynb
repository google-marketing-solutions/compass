{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "convert_cell_0"
      },
      "outputs": [],
      "source": [
        "# Copyright 2022 Google LLC.\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "convert_cell_1"
      },
      "source": [
        "# 2. ML Data Preparation for Lifetime-value (LTV) Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "convert_cell_2"
      },
      "source": [
        "This notebook demonstrates how to create an ML dataset using [ML Data Windowing Pipeline (MLWP)](https://github.com/google/gps_building_blocks/tree/master/py/gps_building_blocks/ml/data_prep/ml_windowing_pipeline) and [Data Visualizastion](https://github.com/google/gps_building_blocks/tree/master/py/gps_building_blocks/ml/data_prep/data_visualizer) modules.\n",
        "\n",
        "* MLWP creates a rich ML dataset by extracting a series of data snapshots over time in a faster, easier and cheaper way.\n",
        "* Data Visualization visualizes input and output data for MLWP for consistency and accuracy to avoid garbage in, garage out situation in modelling.\n",
        "\n",
        "[Google Merchandize Store GA360 dataset](https://support.google.com/analytics/answer/7586738?hl=en) is used as an example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "convert_cell_3"
      },
      "source": [
        "## Requirements\n",
        "\n",
        "* [Google Analytics dataset stored in BigQuery.](https://support.google.com/analytics/answer/3437618?hl=en)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "convert_cell_4"
      },
      "source": [
        "## Install and import required modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "convert_cell_5"
      },
      "outputs": [],
      "source": [
        "# Uncomment to install required python modules\n",
        "# !sh ../utils/setup.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "convert_cell_6"
      },
      "outputs": [],
      "source": [
        "# Add custom utils module to Python environment\n",
        "import os\n",
        "import sys\n",
        "sys.path.append(os.path.abspath(os.pardir))\n",
        "\n",
        "import inspect\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "from gps_building_blocks.cloud.utils import bigquery as bigquery_utils\n",
        "from gps_building_blocks.ml import utils as pipeline_utils\n",
        "from gps_building_blocks.ml.data_prep.ml_windowing_pipeline import ml_windowing_pipeline\n",
        "from gps_building_blocks.ml.data_prep.data_visualizer import instance_visualizer\n",
        "from gps_building_blocks.ml.data_prep.data_visualizer import fact_visualizer\n",
        "from gps_building_blocks.ml.data_prep.data_visualizer import feature_visualizer\n",
        "\n",
        "from utils import helpers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "convert_cell_7"
      },
      "source": [
        "## Configure MLWP module\n",
        "\n",
        "Following copies all MLWP SQL templates to current project directory to make them customizable for your needs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "convert_cell_8"
      },
      "outputs": [],
      "source": [
        "# MWLP SQL template dir for this project\n",
        "MLWP_TEMPLATE_DIR = 'mlwp_templates'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "convert_cell_9"
      },
      "outputs": [],
      "source": [
        "templates_dir = os.path.dirname(inspect.getfile(ml_windowing_pipeline))\n",
        "source_templates = os.path.join(templates_dir, 'templates')\n",
        "!cp -r {source_templates} {MLWP_TEMPLATE_DIR}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "convert_cell_10"
      },
      "source": [
        "Next, set up GCP project details and MWLP dataset configuration. Refer to [this page](https://github.com/google/gps_building_blocks/tree/master/py/gps_building_blocks/ml/data_prep/ml_windowing_pipeline) for more details on MWLP configurable parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "convert_cell_11"
      },
      "outputs": [],
      "source": [
        "configs = helpers.get_configs('config.yaml')\n",
        "source_configs, dest_configs = configs.source, configs.destination\n",
        "\n",
        "# GCP project ID\n",
        "PROJECT_ID = dest_configs.project_id\n",
        "# BigQuery dataset name\n",
        "DATASET_NAME = dest_configs.dataset_name\n",
        "# BigQuery table name containing the original data\n",
        "# e.x. bigquery-public-data.google_analytics_sample.ga_sessions_* for\n",
        "# Google Merchandize Store GA360 dataset\n",
        "SOURCE_TABLE_PATH = (f'{source_configs.project_id}'\n",
        "                    f'.{source_configs.dataset_name}'\n",
        "                    f'.{source_configs.table_name}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "convert_cell_12"
      },
      "outputs": [],
      "source": [
        "# To distinguish the seperate runs of the MWLP\n",
        "RUN_ID = '01'\n",
        "\n",
        "# First data snapshot date in YYYY-MM-DD format\n",
        "SNAPSHOT_START_DATE = '2016-11-17'\n",
        "# Last data snapshot date in YYYY-MM-DD format\n",
        "SNAPSHOT_END_DATE = '2017-07-01'\n",
        "# Sliding window length between shapshots\n",
        "SLIDE_INTERVAL_IN_DAYS = 7\n",
        "# The days from prediction window starts in relation to the snapshot date\n",
        "PREDICTION_WINDOW_GAP_IN_DAYS = 1\n",
        "# The length of the prediction window in days\n",
        "PREDICTION_WINDOW_SIZE_IN_DAYS = 14\n",
        "# The days from lookback window ends in relation to the snapshot date\n",
        "LOOKBACK_WINDOW_GAP_IN_DAYS = 1\n",
        "# The length of the prediction window in days\n",
        "LOOKBACK_WINDOW_SIZE_IN_DAYS = 30\n",
        "\n",
        "# Name of the pdf file containing output instance table plots\n",
        "INSTANCE_TABLE_PLOTS_FILE = 'instance_plots.pdf'\n",
        "# Name of the pdf file containing output numerical fact plots\n",
        "NUMERICAL_FACTS_PLOTS_FILE = 'numerical_fact_plots.pdf'\n",
        "# Name of the pdf file containing output categorical fact plots\n",
        "CATEGORICAL_FACTS_PLOTS_FILE = 'categorical_fact_plots.pdf'\n",
        "# Name of the pdf file containing output feature plots\n",
        "FEATURE_PLOT_FILES = 'feature_plots.pdf'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "convert_cell_13"
      },
      "source": [
        "## Run MLWP and Data Visualization\n",
        "\n",
        "The following steps are executed below to create and visualize the ML dataset:\n",
        "\n",
        "*   Step 1: Run Data Extraction Pipeline\n",
        "*   Step 2: Run Data Exploration Pipeline\n",
        "*   Step 3: Visualize Instances and Facts\n",
        "*   Step 4: Run Windowing Pipeline\n",
        "*   Step 5: Run Feature Generation Pipeline\n",
        "*   Step 6: Visualize Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "convert_cell_14"
      },
      "source": [
        "### Step 1. Run Data Extraction Pipeline\n",
        "\n",
        "This step extracts and formats the original data from the BigQuery table into\n",
        "several temporary tables for further processing.\n",
        "\n",
        "This step first requires to update the following sql templates files in the local /template directory:\n",
        "*   For GA360 data:\n",
        "    * to define the label definition: [*conversions_google_analytics_regression.sql*](https://github.com/google/gps_building_blocks/blob/master/py/gps_building_blocks/ml/data_prep/ml_windowing_pipeline/templates/conversions_google_analytics_regression.sql) and [*prediction_window_conversions_to_label_regression.sql*](https://github.com/google/gps_building_blocks/blob/master/py/gps_building_blocks/ml/data_prep/ml_windowing_pipeline/templates/prediction_window_conversions_to_label_regression.sql) (optional)\n",
        "    * to define the variables to extract (optional): [*sessions_google_analytics.sql*](https://github.com/google/gps_building_blocks/blob/master/py/gps_building_blocks/ml/data_prep/ml_windowing_pipeline/templates/sessions_google_analytics.sql)\n",
        "\n",
        "*   For Firebase data:\n",
        "    * to define the label definition: [*conversions_firebase.sql*](https://github.com/google/gps_building_blocks/blob/master/py/gps_building_blocks/ml/data_prep/ml_windowing_pipeline/templates/conversions_firebase.sql) and [*prediction_window_conversions_to_label_regression.sql*](https://github.com/google/gps_building_blocks/blob/master/py/gps_building_blocks/ml/data_prep/ml_windowing_pipeline/templates/prediction_window_conversions_to_label_regression.sql) (optional)\n",
        "    * to define the variables to extract (optional): [*sessions_firebase.sql*](https://github.com/google/gps_building_blocks/blob/master/py/gps_building_blocks/ml/data_prep/ml_windowing_pipeline/templates/sessions_firebase.sql)\n",
        "\n",
        "For example, the following code derived from  [*conversions_google_analytics_regression.sql*](https://github.com/google/gps_building_blocks/blob/master/py/gps_building_blocks/ml/data_prep/ml_windowing_pipeline/templates/conversions_google_analytics_regression.sql) file specifies the variable and value used to define a numerical label counting the life-time value of each client by summing up completed purchase actions.\n",
        "\n",
        "```sql\n",
        "CREATE OR REPLACE TABLE `{{conversions_table}}`\n",
        "AS (\n",
        "  SELECT DISTINCT\n",
        "    IFNULL(NULLIF(GaTable.clientId, ''), GaTable.fullVisitorId) AS user_id,\n",
        "    TIMESTAMP_SECONDS(GaTable.visitStartTime) AS conversion_ts,\n",
        "    1 AS label\n",
        "  FROM\n",
        "    `{{analytics_table}}` AS GaTable, UNNEST(GaTable.hits) AS hits\n",
        "  WHERE\n",
        "    hits.eCommerceAction.action_type = '6'  -- Google Analytics code for \"Completed purchase\"\n",
        ");\n",
        "```\n",
        "\n",
        "For example, the following SQL code (the default setting) in the [*prediction_window_conversions_to_label_regression.sql*](https://github.com/google/gps_building_blocks/blob/master/py/gps_building_blocks/ml/data_prep/ml_windowing_pipeline/templates/prediction_window_conversions_to_label_regression.sql) can be used to create a numerical label for the prediction window where the label is assigned the sum of purchases occurred in the prediction window.\n",
        "\n",
        "```sql\n",
        "IFNULL(\n",
        "  (\n",
        "    SELECT SUM(Conversions.label)\n",
        "    FROM UNNEST(PredictionWindowConversions.conversions) AS Conversions\n",
        "  ), 0)\n",
        "```\n",
        "\n",
        "Then run data extraction pipeline to extract variables and the label as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "convert_cell_15"
      },
      "outputs": [],
      "source": [
        "data_extract_params = {\n",
        " 'project_id': PROJECT_ID,\n",
        " 'dataset_id': DATASET_NAME,\n",
        " 'analytics_table': SOURCE_TABLE_PATH,\n",
        " 'conversions_sql': 'conversions_google_analytics_regression.sql', # or conversions_firebase_regression.sql\n",
        " 'sessions_sql': 'sessions_google_analytics.sql', # or sessions_firebase.sql\n",
        " 'templates_dir': MLWP_TEMPLATE_DIR,\n",
        " 'run_id': RUN_ID\n",
        "}\n",
        "ml_windowing_pipeline.run_data_extraction_pipeline(data_extract_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "convert_cell_16"
      },
      "source": [
        "### Step 2. Run Data Exploration Pipeline\n",
        "\n",
        "This step outputs facts and instances into BigQuery tables (*numeric_facts_{run_id}*, *categorical_facts_{run_id}* and *instances_{run_id}*) for data exploration and analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "convert_cell_17"
      },
      "outputs": [],
      "source": [
        "data_explo_params = {\n",
        " 'project_id': PROJECT_ID,\n",
        " 'dataset_id': DATASET_NAME,\n",
        " 'analytics_table': SOURCE_TABLE_PATH,\n",
        " 'snapshot_start_date': SNAPSHOT_START_DATE,\n",
        " 'snapshot_end_date': SNAPSHOT_END_DATE,\n",
        " 'slide_interval_in_days': SLIDE_INTERVAL_IN_DAYS,\n",
        " 'prediction_window_gap_in_days': PREDICTION_WINDOW_GAP_IN_DAYS,\n",
        " 'prediction_window_size_in_days': PREDICTION_WINDOW_SIZE_IN_DAYS,\n",
        " 'templates_dir': MLWP_TEMPLATE_DIR,\n",
        " 'run_id': RUN_ID,\n",
        " 'prediction_window_conversions_to_label_sql': 'prediction_window_conversions_to_label_regression.sql'\n",
        "}\n",
        "\n",
        "ml_windowing_pipeline.run_data_exploration_pipeline(data_explo_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "convert_cell_18"
      },
      "source": [
        "### Step 3. Visualize Instances and Facts\n",
        "\n",
        "This step visualizes instances and facts\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "convert_cell_19"
      },
      "source": [
        "### Step 3.1. Visualize Instances\n",
        "\n",
        "Instance table in BigQuery contains all the instances (e.g. users) selected for each snapshot date with some additional information such as their label, days since the first activity and days since the last activity.\n",
        "\n",
        "This step generates the following plots:\n",
        "* plots with the number of total instances, number of positive instances and proportion of positive instances for each snapshot. These plots are helpful to understand how the label is distributed over time, any seasonality and trends, and whether there are any inconsistencies. Based on this we can drop specific periods of snapshots having any data issues and consider what additional features to add to capture the seasonality or any trends of the label over time.\n",
        "* class specific distribution plots for the *days_since_first_activity* and *days_since_latest_activity* features in the Instance table. From these plots, we can determine a good lookback window period to use to create features and whether it’s worth only using customers having a particular history and recency for modeling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "convert_cell_21"
      },
      "outputs": [],
      "source": [
        "bq_utils = bigquery_utils.BigQueryUtils(project_id=PROJECT_ID)\n",
        "instance_viz = instance_visualizer.InstanceVisualizer(\n",
        "        bq_client=bq_utils.client,\n",
        "        instance_table_path=f'{PROJECT_ID}.{DATASET_NAME}.instances_{RUN_ID}',\n",
        "        num_instances=100000, # no. radom instances used for ploting\n",
        "        label_column='label', # name of the label column\n",
        "        label_type='numerical', # label type\n",
        "        )\n",
        "\n",
        "instance_plots = instance_viz.plot_instances()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "convert_cell_22"
      },
      "outputs": [],
      "source": [
        "# Save the plots to a pdf file\n",
        "instance_plots_pdf = f'{RUN_ID}_{INSTANCE_TABLE_PLOTS_FILE}'\n",
        "helpers.save_to_pdf(filename=instance_plots_pdf, plots=instance_plots)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "convert_cell_23"
      },
      "source": [
        "### Step 3.2. Visualize Facts Table\n",
        "\n",
        "Facts table in BigQuery is created by the Data Exploration Pipeline of ML Windowing Pipeline, which contains the original GA variable transformed into facts format containing *user_id*, *timestamp*, *fact_name* and *fact_value* columns.\n",
        "\n",
        "This step generates plots of numerical and categorical fact variables, which can be used to explore their validity and distribution over time. Based on that we can make decisions such as which facts variables (and which levels in categorical fact variables) to use to generate features in the following steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "convert_cell_24"
      },
      "outputs": [],
      "source": [
        "fact_viz = fact_visualizer.FactVisualizer(\n",
        "        bq_client=bq_utils.client,\n",
        "        numerical_facts_table_path=f'{PROJECT_ID}.{DATASET_NAME}.numeric_facts_{RUN_ID}',\n",
        "        categorical_facts_table_path=f'{PROJECT_ID}.{DATASET_NAME}.categorical_facts_{RUN_ID}',\n",
        "        number_top_categories=5 # No. top categories to explore for categorical variables\n",
        "        )\n",
        "\n",
        "numerical_fact_plots = fact_viz.plot_numerical_facts()\n",
        "categorical_fact_plots = fact_viz.plot_categorical_facts()\n",
        "plt.close('all') # Don't show plots in notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "convert_cell_25"
      },
      "outputs": [],
      "source": [
        "numerical_fact_plots_pdf = f'{RUN_ID}_{NUMERICAL_FACTS_PLOTS_FILE}'\n",
        "categorical_fact_plots_pdf = f'{RUN_ID}_{CATEGORICAL_FACTS_PLOTS_FILE}'\n",
        "\n",
        "# Save the plots to a pdf files\n",
        "for filename, plot_list in zip(\n",
        "  [numerical_fact_plots_pdf, categorical_fact_plots_pdf],\n",
        "  [numerical_fact_plots, categorical_fact_plots]):\n",
        "  helpers.save_to_pdf(filename, plot_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "convert_cell_26"
      },
      "source": [
        "### Step 4. Run Data Windowing Pipeline\n",
        "\n",
        "This step segments the user data into multiple, potentially overlapping time windows, with each window containing a lookback window and a prediction window. This generates an internal table in BigQuery (*windows_{run_id}*) for further processing.\n",
        "\n",
        "The windows can be defined in two ways:\n",
        "* based on calendar dates and a sliding window. This is implemented in the *sliding_windows.sql* and used as the default.\n",
        "* based on each session of each user. This is implemented in the *session_windows.sql* and you can use the *windows_sql* parameter to specify it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "convert_cell_27"
      },
      "outputs": [],
      "source": [
        "windowing_params = {\n",
        " 'project_id': PROJECT_ID,\n",
        " 'dataset_id': DATASET_NAME,\n",
        " 'snapshot_start_date': SNAPSHOT_START_DATE,\n",
        " 'snapshot_end_date': SNAPSHOT_END_DATE,\n",
        " 'slide_interval_in_days': SLIDE_INTERVAL_IN_DAYS,\n",
        " 'prediction_window_gap_in_days': PREDICTION_WINDOW_GAP_IN_DAYS,\n",
        " 'prediction_window_size_in_days': PREDICTION_WINDOW_SIZE_IN_DAYS,\n",
        " 'lookback_window_gap_in_days': LOOKBACK_WINDOW_GAP_IN_DAYS,\n",
        " 'lookback_window_size_in_days': LOOKBACK_WINDOW_SIZE_IN_DAYS,\n",
        " 'run_id': RUN_ID,\n",
        " 'prediction_window_conversions_to_label_sql': 'prediction_window_conversions_to_label_regression.sql'\n",
        "}\n",
        "\n",
        "ml_windowing_pipeline.run_windowing_pipeline(windowing_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "convert_cell_28"
      },
      "source": [
        "### Step 5. Run Feature Generation Pipeline\n",
        "\n",
        "This step generates features from the windows of data computed in Data Windowing Pipeline and outputs to *features_{run_id}* table in BigQuery.\n",
        "\n",
        "In this step, we can select the variables, feature types (aggregation functions) and input values based on prior knowledge or the exploration of facts done at the Fact Visualization step.\n",
        "\n",
        "For numerical variables, the following feature types (aggregated functions) are supported:\n",
        "* Sum: sum of all the values over the lookback window\n",
        "* Average: average of all the values over the lookback window\n",
        "* Min: minimum of all the values over the lookback window\n",
        "* Max: maximum of all the values over the lookback window\n",
        "\n",
        "These options expect a semi-colon separated list of numerical fact names to create the corresponding features (e.x. `sum_values:'variable_1;variable_2,...'`).\n",
        "\n",
        "For categorical variables, the following feature types (aggregated functions) are supported:\n",
        "* Counts: total occurrence of each category\n",
        "* Proportions: proportion of occurance of each category\n",
        "* Latest value: the latest category value\n",
        "* Mode value: the most frequent category value\n",
        "\n",
        "These options expect a semi-colon separated list of categorical Feature Options (`\u003cfeature_option1\u003e;\u003cfeature_option2\u003e;\u003cfeature_option3\u003e`). Each Feature Option should contain a categorical fact name, a list of categorical values to consider and a default value. The default value is specified to use the common value for any value not on the provided list. Feature Option = `\u003cfact_name\u003e:[\u003cvalue1\u003e, …,\u003cvalueN\u003e]:[\u003cdefault_value\u003e]`. (e.x. `count_values':'trafficSource_medium:[cpm,cpc,referral,affiliate,organic]:[Other];device_isMobile:[false,true]:[Other]'`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "convert_cell_29"
      },
      "outputs": [],
      "source": [
        "features_params = {\n",
        " 'project_id': PROJECT_ID,\n",
        " 'dataset_id': DATASET_NAME,\n",
        " 'features_sql': 'features_from_input.sql',\n",
        " 'sum_values': 'totals_visits;totals_pageviews',\n",
        " 'avg_values': 'totals_visits;totals_pageviews',\n",
        " 'min_values': 'totals_visits;totals_pageviews',\n",
        " 'max_values': 'totals_visits;totals_pageviews',\n",
        " 'count_values': 'trafficSource_medium:[cpm,cpc,referral,affiliate,organic]:[Other];device_isMobile:[false,true]:[Other]',\n",
        " 'latest_values': 'trafficSource_medium:[cpm,cpc,referral,affiliate,organic]:[Other];device_isMobile:[false,true]:[Other]',\n",
        " 'proportions_values': 'trafficSource_medium:[cpm,cpc,referral,affiliate,organic]:[Other];device_isMobile:[false,true]:[Other]',\n",
        " 'mode_values': 'trafficSource_medium:[cpm,cpc,referral,affiliate,organic]:[Other];device_isMobile:[false,true]:[Other]',\n",
        " 'templates_dir': MLWP_TEMPLATE_DIR,\n",
        " 'run_id': RUN_ID,\n",
        "}\n",
        "\n",
        "ml_windowing_pipeline.run_features_pipeline(features_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Avn-O2h9UOCH"
      },
      "source": [
        "### (Optional) Step 6. Merge with Customized Features\n",
        "This step merges the generated feature table above and any additional customized feature table (eg. user_id level aggregated CRM data), in order to generate the final dataset for modeling. To ensure the generated dataset is fair for modeling and does not contain data leakage, the customized feature table should be generated and merged for each ML instance defined by user_id and [snapshot_ts](https://github.com/google/gps_building_blocks/blob/master/py/gps_building_blocks/ml/data_prep/ml_windowing_pipeline/templates/instances.sql#L41).\n",
        "\n",
        "In this example, we assume that:\n",
        "- Model labels (LTV) is generated with MLWP dataset based on GA / Firebase data by previous sessions. To use label from CRM data, please drop the label column in MLWP dataset.\n",
        "- The lookback window to create features is defined based on parameters in this notebook (`LOOKBACK_WINDOW_SIZE_IN_DAYS` and `LOOKBACK_WINDOW_GAP_IN_DAYS`). To use customized feature window based on CRM data (egf. different window size for each user), the MLWP dataset needs to be generated with the same time window.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Avn-O1h9UOCH"
      },
      "outputs": [],
      "source": [
        "# Please modify the BigQuery table paths here if necessary.\n",
        "mlwp_feature_table = f'{PROJECT_ID}.{DATASET_NAME}.features_{RUN_ID}'\n",
        "custom_features_table = f'{PROJECT_ID}.{DATASET_NAME}.crm_feature'\n",
        "merged_features_table = f'{PROJECT_ID}.{DATASET_NAME}.merged_feature'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkit4DFbUK2A"
      },
      "outputs": [],
      "source": [
        "query = pipeline_utils.configure_sql(\n",
        "  sql_path=os.path.join('../utils/templates', 'customized_dataset_merge.sql'),\n",
        "  query_params={\n",
        "      'merged_dataset_table': merged_features_table,\n",
        "      'mlwp_feature_table': mlwp_feature_table,\n",
        "      'user_dataset_table': custom_features_table,\n",
        "      'crm_data_date_start': '2020-01-01',\n",
        "      'crm_data_date_end': '2020-07-01',\n",
        "      'crm_user_id': 'user_id',\n",
        "      'crm_snapshot_ts': 'snapshot_ts'\n",
        "  })\n",
        "\n",
        "bq_utils.run_query(query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "convert_cell_30"
      },
      "source": [
        "### Step 7. Visualize Features\n",
        "\n",
        "This step visualizes the statistics calculated from the Features table in Big Query. The plots include class-specific distribution plots of numerical and categorical features, which can be used to explore the validity of the features and potentially identify issues such as label leakage, and the distribution of the features over time helping to understand the consistency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "convert_cell_31"
      },
      "outputs": [],
      "source": [
        "# Read in Features table schema to select numerical and categorical feature\n",
        "# names.\n",
        "# Please replace the table name (merged_features_table) if Step 6 is executed.\n",
        "sql = (\"SELECT column_name, data_type \"\n",
        "       f\"FROM `{PROJECT_ID}.{DATASET_NAME}`.INFORMATION_SCHEMA.COLUMNS \"\n",
        "       f\"WHERE table_name='features_{RUN_ID}';\")\n",
        "\n",
        "features_schema = bq_utils.run_query(sql).to_dataframe()\n",
        "features_schema.columns = ['column_name', 'type']\n",
        "print(features_schema.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "convert_cell_32"
      },
      "outputs": [],
      "source": [
        "# Select numerical and categorical feature names\n",
        "numerical_features = (list(\n",
        "    features_schema[features_schema['type'].\n",
        "                    isin(['INT64','FLOAT64'])]['column_name']))\n",
        "categorical_features = list(\n",
        "    features_schema[features_schema['type'].\n",
        "                    isin(['STRING'])]['column_name'])\n",
        "\n",
        "# Columns to remove if any\n",
        "to_remove = ['user_id', 'label']\n",
        "numerical_features = [v for v in numerical_features if v not in to_remove]\n",
        "categorical_features = [v for v in categorical_features if v not in to_remove]\n",
        "\n",
        "print('No. of numerical features:', len(numerical_features))\n",
        "print('No. of categorical features:', len(categorical_features))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "convert_cell_33"
      },
      "outputs": [],
      "source": [
        "# Plot features\n",
        "# Please replace the table name (merged_features_table) if Step 6 is executed.\n",
        "feature_viz = feature_visualizer.FeatureVisualizer(\n",
        "        bq_client=bq_utils.client,\n",
        "        features_table_path=f'{PROJECT_ID}.{DATASET_NAME}.features_{RUN_ID}',\n",
        "        numerical_features=numerical_features,\n",
        "        categorical_features=categorical_features,\n",
        "        label_column='label', # name of the label column\n",
        "        label_type='numerical', # label type\n",
        "        )\n",
        "\n",
        "feature_plots = feature_viz.plot_features()\n",
        "plt.close('all') # Don't show plots in notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "convert_cell_34"
      },
      "outputs": [],
      "source": [
        "# Save the plots to a pdf files\n",
        "feature_plots_pdf = f'{RUN_ID}_{FEATURE_PLOT_FILES}'\n",
        "helpers.save_to_pdf(filename=feature_plots_pdf, plots=feature_plots)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "2.ml_data_preparation.ipynb",
      "provenance": [
        {
          "file_id": "/piper/depot/google3/third_party/professional_services/solutions/compass/packages/ltv/02.ml_data_preparation.ipynb",
          "timestamp": 1639018324150
        }
      ]
    },
    "environment": {
      "name": "common-cpu.m73",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/base-cpu:m73"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
