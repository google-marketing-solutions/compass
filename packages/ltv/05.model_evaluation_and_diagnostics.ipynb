{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IaSc1TJ3fyx"
      },
      "source": [
        "## Disclaimer and license\n",
        "Copyright 2022 Google LLC. This solution, including any related sample code or data, is made available on an “as is,” “as available,” and “with all faults” basis, solely for illustrative purposes, and without warranty or representation of any kind. This solution is experimental, unsupported and provided solely for your convenience. Your use of it is subject to your agreements with Google, as applicable, and may constitute a beta feature as defined under those agreements. To the extent that you make any data available to Google in connection with your use of the solution, you represent and warrant that you have all necessary and appropriate rights, consents and permissions to permit Google to use and process that data. By using any portion of this solution, you acknowledge, assume and accept all risks, known and unknown, associated with its usage, including with respect to your deployment of any portion of this solution in your systems, or usage in connection with your business, if at all.\n",
        "\n",
        "Copyright 2022 Google LLC\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
        "\n",
        "http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxxXTC7i3rlf"
      },
      "source": [
        "# 5.Model Evaluation and Diagnostics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afS2AgAH3uiH"
      },
      "source": [
        "This notebook demonstrates the evaluation of a LTV regression model by using the\n",
        "[Regression Diagnostics](https://github.com/google/gps_building_blocks/blob/master/py/gps_building_blocks/ml/diagnostics/regression.py)\n",
        "module.\n",
        "\n",
        "This evaluation consists of:\n",
        "* Model performance with respect to a variety of metrics.\n",
        "* Plots to understand the model performance to design media experiments.\n",
        "* Model insights (the relationship between features and the prediction values) helping to generate new business insights.\n",
        "* Insights helping to diagnose the model to make sure it is reasonable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ag1Rhu93RxU"
      },
      "source": [
        "## Requirements\n",
        "The model and the testing dataset should be available in GCP BigQuery."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bw1I4yQ_32vj"
      },
      "source": [
        "## Install and import required modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uiYQxYEc3b2Z"
      },
      "outputs": [],
      "source": [
        "# Uncomment to install required python modules\n",
        "#!sh ../utils/setup.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78JIAhwm8E3R"
      },
      "outputs": [],
      "source": [
        "# Add custom utils module to Python environment\n",
        "import os\n",
        "import sys\n",
        "sys.path.append(os.path.abspath(os.pardir))\n",
        "\n",
        "from gps_building_blocks.cloud.utils import bigquery as bigquery_utils\n",
        "from gps_building_blocks.ml.diagnostics import regression\n",
        "from utils import helpers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYoWaBBY3rGw"
      },
      "source": [
        "## Set parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ujIclxLF8Kxj"
      },
      "outputs": [],
      "source": [
        "configs = helpers.get_configs('config.yaml')\n",
        "dest_configs = configs.destination\n",
        "\n",
        "# GCP project ID\n",
        "PROJECT_ID = dest_configs.project_id\n",
        "# Name of the BigQuery dataset\n",
        "DATASET_NAME = dest_configs.dataset_name\n",
        "# To distinguish the seperate runs of the training pipeline\n",
        "RUN_ID = '01'\n",
        "# BigQuery table name containing model development dataset\n",
        "FEATURES_DEV_TABLE = f'features_dev_table_{RUN_ID}'\n",
        "# BigQuery table name containing out of time test dataset\n",
        "FEATURES_TEST_TABLE = f'features_test_table_{RUN_ID}'\n",
        "# BQML model name to save in BigQuery\n",
        "BQML_MODEL_NAME = f'ltv_model_bqml_{RUN_ID}'\n",
        "# BigQuery table name containing the testing predictions dataset\n",
        "PREDICTION_TABLE = 'predictions_table'\n",
        "# Name of the column in the prediction table with the predicted label\n",
        "PREDICTED_LABEL = 'predicted_label'\n",
        "# Name of the column in the prediction table with the actual label\n",
        "ACTUAL_LABEL = 'label'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dpU-EH8W_Kgh"
      },
      "outputs": [],
      "source": [
        "# Initialize BigQuery client.\n",
        "bq_utils = bigquery_utils.BigQueryUtils(project_id=PROJECT_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rY0VTj0O8v77"
      },
      "source": [
        "## Create test dataset for prediction (if not predicted)\n",
        "In this step, it creates the prediction dataset using test dataset for model performance diagnostic. This step is skippable if the test dataset for prediction already exists.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_ZrTu4d82R9"
      },
      "outputs": [],
      "source": [
        "prediction_query = f\"\"\"\n",
        "  CREATE OR REPLACE TABLE `{PROJECT_ID}.{DATASET_NAME}.{PREDICTION_TABLE}`\n",
        "  AS (\n",
        "    SELECT *\n",
        "    FROM ML.PREDICT(MODEL `{PROJECT_ID}.{DATASET_NAME}.{BQML_MODEL_NAME}`,\n",
        "                    TABLE `{PROJECT_ID}.{DATASET_NAME}.{FEATURES_TEST_TABLE}`)\n",
        "  );\n",
        "\"\"\"\n",
        "print(prediction_query)\n",
        "bq_utils.run_query(prediction_query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPJI3_SB9HZh"
      },
      "source": [
        "## Read test dataset for prediction (if already predicted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y18Q7O7_9L9I"
      },
      "source": [
        "In this step, we assume the test dataset containing both prediction and actual values is available in BigQuery."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tK-xt1x69GtB"
      },
      "outputs": [],
      "source": [
        "sql = f\"\"\"\n",
        "  SELECT *\n",
        "  FROM `{PROJECT_ID}.{DATASET_NAME}.{PREDICTION_TABLE}`\n",
        "  ;\n",
        "\"\"\"\n",
        "df_pred_test = bq_utils.run_query(sql).to_dataframe()\n",
        "df_pred_test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3kzS9RP9DMg"
      },
      "outputs": [],
      "source": [
        "# Change negative predictions to 0.0 to avoid metrics calculation error.\n",
        "df_pred_test[PREDICTED_LABEL] = [\n",
        "    0.0 if predicted_label \u003c 0 else predicted_label\n",
        "    for predicted_label in df_pred_test[PREDICTED_LABEL]\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4i6623PC-iCf"
      },
      "source": [
        "## Run regression diagnostics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGMNCjNx_aM2"
      },
      "source": [
        "### Calculate performance metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73VJ8J9bBNz2"
      },
      "source": [
        "To check the regression model performance, we calculated the following metrics for diagnostic:\n",
        "- Mean squared error: a risk metric corresponding to the expected value of the squared error, calculated by taking the mean of squared error.\n",
        "- Root mean squared error: the root value of the mean squared error.\n",
        "- Mean absolute error: a risk metric corresponding to the expected of the absolute value, calculated by taking the mean of absolute error.\n",
        "- Mean absolute percentage error: an evaluation metric for regression problems, sensitive to relative errors, calculated by taking the mean of the absolute percentage error.\n",
        "- R-squared: coefficient of determination, representing the proportion of variance that has been explained by the independent variables in the model.\n",
        "- Pearson correlation: a correlation metric between actual and predicted labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0LElmLQgD62i"
      },
      "outputs": [],
      "source": [
        "perf_metrics = regression.calc_performance_metrics(\n",
        "    labels=df_pred_test[ACTUAL_LABEL],\n",
        "    predictions=df_pred_test[PREDICTED_LABEL].values,\n",
        "    decimal_points=4)\n",
        "\n",
        "print(perf_metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzkiLPyL--eV"
      },
      "source": [
        "### Scatter Plots of prediction values and residuals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRGLMOCWlo1F"
      },
      "source": [
        "The following function plots:\n",
        "1. the scatter plot of actual values versus prediction values\n",
        "2. the scatter plot of residuals versus predicted values\n",
        "From the first plot, we can see how prediction values differ from actual values, whether there exists strong correlation between the predictions and actual results. In the second plot, the residual plot, we can see how the residuals deviate from the line at zero since the residual equals to the actual value minus the prediction value. Ideal residual plots would be symmetrically distributed and cluster close to y=0 value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2yeG6W1IHHsp"
      },
      "outputs": [],
      "source": [
        "regression.plot_prediction_residuals(\n",
        "    labels=df_pred_test[ACTUAL_LABEL],\n",
        "    predictions=df_pred_test[PREDICTED_LABEL].values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vF4L8pWWAZUR"
      },
      "source": [
        "### Calculate performance metrics for the bins of the prediction values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8USUxFvRrrGN"
      },
      "source": [
        "The following function calculates performance metrics for each bin of the predictions. The default number of the bins of the prediction values is a parameter and here we use 3 as an example. The calculation is firstly rankd the prediction values from the highest to the lowest and then devide the predictions into 3 bins such that the first bin contains the highest 33.33% of the predictions and the second bin contains the next 33.33% of the predictions and so on. We suggest to start from small number of bins like 3 and tune the parameter to larger number like 10 check the performance. Due to the distribution of the prediction and actual label, if the proportion of zero is too high then we will encounter all zeros in certain bins, which will cause issue in performance metrics calculation. The following metrics are calculated for each bin:\n",
        "- mean_label: Mean of actual values in the bin.\n",
        "- mean_prediction: Mean of predictions in the bin.\n",
        "- rmse: Root mean squared error.\n",
        "- mape: Mean absolute percentage error.\n",
        "- corr: Pearson correlation coefficient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Z6xfBSpp29B"
      },
      "outputs": [],
      "source": [
        "bin_metrics = regression.calc_reg_bin_metrics(\n",
        "    labels=df_pred_test[ACTUAL_LABEL],\n",
        "    predictions=df_pred_test[PREDICTED_LABEL].values,\n",
        "    number_bins=3)\n",
        "\n",
        "bin_metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrmsaPP_qwgN"
      },
      "source": [
        "### Plot performance metrics for the bins of the prediction values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRX2klH9En2r"
      },
      "source": [
        "The following function plots performance metrics in each bin using bin_metrics calculated in last section. These plots allow us to have better understanding of predictions in the test dataset. In the first subplot, it shows the mean of actual and prediction values in each bin. An ideal plot would be the height of bars of both prediction and actual values decrease as the number of the bin increases. In the rest three plots (MAPE, RMSE, corr), the evaluation metric has same interpretation as in the regression models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTI_WfClrECG"
      },
      "outputs": [],
      "source": [
        "regression.plot_reg_bin_metrics(\n",
        "    bin_metrics=bin_metrics,\n",
        "    fig_width=25,\n",
        "    fig_height=30);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLevazIOAPGJ"
      },
      "source": [
        "### Plot actual distribution over the bins of the prediction values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuUc24Sf-Mq4"
      },
      "source": [
        "The following function plots the distribution of actual label values over the bins of the prediction values. The plot provides better insight for the action value distribution in each bin of the predictions. From the boxplots we can have a good understanding of the median, the spread, the interquartile range and outliers if any in each prediction bin, expecting a monotonically decreasing trend over the bins from the highest to the lowest predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XwOiQ8nazTCy"
      },
      "outputs": [],
      "source": [
        "regression.plot_binned_preds_labels(\n",
        "    labels=df_pred_test[ACTUAL_LABEL],\n",
        "    predictions=df_pred_test[PREDICTED_LABEL].values,\n",
        "    number_bins=3);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6h32ZIlNAiYp"
      },
      "source": [
        "### Confusion matrix of the actual vs predicted bins"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_qdKFa0_8Bq"
      },
      "source": [
        "The function helps to visualize and compare the distribution of the bins of both the actual value and the predicted value. It does the following:\n",
        "\n",
        "* Sort both actual value and predicted value in the descending order and divide them into number_bins bins.\n",
        "* Calculate confusion matrix and normalize it over the true labels when the parameter normalize = true. It can also normalize the confusion matrix over the predictions or over all population. It takes the values 'pred' and 'all' respectively.\n",
        "* Plot heatmap of the actual and predited bins from the highest to the lowest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGKgN-4La6Z0"
      },
      "outputs": [],
      "source": [
        "regression.plot_confusion_matrix_bin_heatmap(\n",
        "    labels=df_pred_test[ACTUAL_LABEL],\n",
        "    predictions=df_pred_test[PREDICTED_LABEL].values,\n",
        "    number_bins=3,\n",
        "    normalize='true');"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "5.model_evaluation_and_diagnostics.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
