{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87cd9c05",
      "metadata": {
        "id": "87cd9c05"
      },
      "outputs": [],
      "source": [
        "# Copyright 2022 Google LLC.\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e563d2a",
      "metadata": {
        "id": "2e563d2a"
      },
      "source": [
        "This notebook demonstrates the generation of a propensity audience for a remarketing use case. It relies on the sample size calculations from the [6.media_experiment_design.ipynb](https://source.corp.google.com/piper///depot/google3/third_party/professional_services/solutions/compass/packages/propensity/6.media_experiment_design.ipynb) notebook to create the Test and Control audiences which are written to a new BigQuery table. This data can then be uploaded via measurement protocol to GA and used for the activation with the Google Ads products as demonstrated in [9.audience_upload.ipynb notebook](google3/third_party/professional_services/solutions/compass/packages/propensity/9.audience_upload.ipynb) notebook.\n",
        "\n",
        "**Requirements:**\n",
        "\n",
        "\n",
        "* An already scored dataset from the [7.batch_scoring.ipynb](google3/third_party/professional_services/solutions/compass/packages/propensity/7.batch_scoring.ipynb) notebook: this is the model prediction dataset containing ML prediction for each `user_id` and `snapshot_ts`, from which we create the remarketing audience.\n",
        "* Statistical sample size calculations from the [6.media_experiment_design.ipynb](https://source.corp.google.com/piper///depot/google3/third_party/professional_services/solutions/compass/packages/propensity/6.media_experiment_design.ipynb) notebook for each propensity audience group."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89b380f5",
      "metadata": {
        "id": "89b380f5"
      },
      "source": [
        "## Install and import required modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5c220df",
      "metadata": {
        "id": "f5c220df"
      },
      "outputs": [],
      "source": [
        "# Uncomment to install required python modules\n",
        "# !sh ../utils/setup.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2f61cbb",
      "metadata": {
        "id": "a2f61cbb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "from gps_building_blocks.cloud.utils import bigquery as bigquery_utils\n",
        "\n",
        "import utils"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51254ad9",
      "metadata": {
        "id": "51254ad9"
      },
      "source": [
        "## Set parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PU1T6ipaFQ38",
      "metadata": {
        "id": "PU1T6ipaFQ38"
      },
      "outputs": [],
      "source": [
        "configs = utils.get_configs('config.yaml')\n",
        "dest_configs = configs.destination\n",
        "\n",
        "# GCP project ID\n",
        "PROJECT_ID = dest_configs.project_id\n",
        "# Name of BigQuery dataset,\n",
        "# destination for created tables for modelling and activation.\n",
        "DATASET_NAME = dest_configs.dataset_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89618c5c",
      "metadata": {
        "id": "89618c5c"
      },
      "outputs": [],
      "source": [
        "# To distinguish the seperate runs of the MWLP\n",
        "RUN_ID = 'SCORE-01'\n",
        "# BigQuery table name containing the predictions (e.g. generated by\n",
        "# 7.batch_scoring.ipynb notebook)\n",
        "PREDICTIONS_TABLE = f'scored_{RUN_ID}'\n",
        "# Snapshot date to select the ML instances to create the marketing audience in\n",
        "# YYYY-MM-DD format\n",
        "SELECTED_SNAPSHOT_DATE = '2017-06-15'\n",
        "# Name of the column in the predictions table with the predicted label values\n",
        "PREDICTED_LABEL_NAME = 'predicted_label_probs'\n",
        "# Label value for the positive class\n",
        "POSITIVE_CLASS_LABEL = True\n",
        "# Number of propensity audience groups to devide the scored users into\n",
        "# (e.g. 3 bins for High, Medium and Low propensity audience groups)\n",
        "AUDIENCE_GROUPS = 3\n",
        "# Minimum samples sizes to select as the Test and Control groups for each the\n",
        "# propensity audience groups based on the output of the\n",
        "# 6.media_experiment_design.ipynb notebook (following are some example numbers).\n",
        "MIN_SAMPLE_SIZES = [1000, 2000, 3000]\n",
        "# Name of the BigQuery table with exported audience\n",
        "AUDIENCE_EXPORT_TABLE = f'audience_export_{RUN_ID}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4180295a",
      "metadata": {
        "id": "4180295a"
      },
      "outputs": [],
      "source": [
        "bq_utils = bigquery_utils.BigQueryUtils(project_id=PROJECT_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8c96833",
      "metadata": {
        "id": "e8c96833"
      },
      "source": [
        "### Read the prediction dataset\n",
        "\n",
        "In this step, we assume the prediction dataset is available as a BigQuery table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f802c4ea",
      "metadata": {
        "id": "f802c4ea"
      },
      "outputs": [],
      "source": [
        "# SQL for extracting prediction dataset when using BQML.\n",
        "sql = f\"\"\"\n",
        "  SELECT\n",
        "    user_id,\n",
        "    snapshot_ts,\n",
        "    days_since_latest_activity,\n",
        "    days_since_first_activity,\n",
        "    probs.label AS predicted_score_label,\n",
        "    probs.prob AS score\n",
        "  FROM\n",
        "    `{PROJECT_ID}.{DATASET_NAME}.{PREDICTIONS_TABLE}` AS predictions,\n",
        "    UNNEST({PREDICTED_LABEL_NAME}) AS probs\n",
        "  WHERE\n",
        "    probs.label={POSITIVE_CLASS_LABEL}\n",
        "    AND snapshot_ts='{SELECTED_SNAPSHOT_DATE}';\n",
        "\"\"\"\n",
        "print (sql)\n",
        "df_prediction = bq_utils.run_query(sql).to_dataframe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8f61dda",
      "metadata": {
        "id": "a8f61dda"
      },
      "outputs": [],
      "source": [
        "df_prediction.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OmptfkZeN-8k",
      "metadata": {
        "id": "OmptfkZeN-8k"
      },
      "outputs": [],
      "source": [
        "# If required, the users can be filtered by using days_since_latest_activity\n",
        "# (tenure) and days_since_first_activity (recency) columns before creating the\n",
        "# audience groups"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87be2f9c",
      "metadata": {
        "id": "87be2f9c"
      },
      "source": [
        "### Create audience groups\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ff1vBP0zNG6e",
      "metadata": {
        "id": "Ff1vBP0zNG6e"
      },
      "outputs": [],
      "source": [
        "# Separate the users into \u003cAUDIENCE_GROUPS\u003e number of audience groups\n",
        "df_prediction = df_prediction.sort_values(by='score',\n",
        "                                          ascending=False).reset_index()\n",
        "# To avoid duplicate edges of bins we use the index in the qcut function below\n",
        "df_prediction['audience_group'] = pd.qcut(df_prediction.index,\n",
        "                                          q=AUDIENCE_GROUPS, labels=False)\n",
        "# When AUDIENCE_GROUPS=3, audience_group column contains '0', '1' and '2' values\n",
        "# representing 'High', 'Medium' and 'Low' propensity groups respectively\n",
        "\n",
        "# Separate each audience group into Test and Control\n",
        "df_prediction['test_control'] = 'NA'\n",
        "for i in range(len(MIN_SAMPLE_SIZES)):\n",
        "  group = df_prediction[df_prediction['audience_group'] == i]\n",
        "  # Select Control set size based on the minimum sample size\n",
        "  control_user_ids = random.sample(list(group['user_id']), MIN_SAMPLE_SIZES[i])\n",
        "  remaining_user_ids = list(set(group.user_id) - set(control_user_ids))\n",
        "\n",
        "  # Select Test set size based on the minimum sample size\n",
        "  test_user_ids = random.sample(remaining_user_ids, MIN_SAMPLE_SIZES[i])\n",
        "  # Alternatively, select Test set to include all the remaining users as below\n",
        "  # or a subset of users greater than MIN_SAMPLE_SIZES[i] depending on the\n",
        "  # avaialble campaign budget\n",
        "  # test_user_ids = remaining_user_ids\n",
        "\n",
        "  df_prediction.loc[df_prediction['user_id'].isin(test_user_ids),\n",
        "                    'test_control'] = 'Test'\n",
        "  df_prediction.loc[df_prediction['user_id'].isin(control_user_ids),\n",
        "                    'test_control'] = 'Control'\n",
        "\n",
        "# Explore the created audience sizes and statistics of the predicted\n",
        "# probabilities. We should expect to see simillar statistics of probabilities\n",
        "# between each Test and Control pair\n",
        "df_prediction.groupby(['audience_group', 'test_control']).agg(\n",
        "    {'score':['count', 'min', 'mean', 'median', 'max']})\n",
        "\n",
        "# TODO(): Add box plots to visualize the probabilities of Test and\n",
        "# Control groups"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YiEcE2Mzm2N_",
      "metadata": {
        "id": "YiEcE2Mzm2N_"
      },
      "source": [
        "### Write the audience data to a BigQuery table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JnJGGbDbNHAO",
      "metadata": {
        "id": "JnJGGbDbNHAO"
      },
      "outputs": [],
      "source": [
        "df_prediction.to_gbq(\n",
        "    destination_table=f'{DATASET_NAME}.{AUDIENCE_EXPORT_TABLE}',\n",
        "    project_id=PROJECT_ID,\n",
        "    if_exists='replace')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cda943c-4f77-4899-82f0-d33cb9ed7389",
      "metadata": {
        "id": "1cda943c-4f77-4899-82f0-d33cb9ed7389"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "8.audience_generation.ipynb",
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "/piper/depot/google3/third_party/professional_services/solutions/compass/packages/propensity/8a.audience_generation.ipynb?workspaceId=szczecinski:dev_audience_generation::citc",
          "timestamp": 1632737106105
        },
        {
          "file_id": "/piper/depot/google3/third_party/professional_services/solutions/compass/packages/propensity/8a.audience_generation.ipynb?workspaceId=szczecinski:dev_audience_generation::citc",
          "timestamp": 1631631928007
        },
        {
          "file_id": "1Dh3leR4-p532CVEYY613FKWH9bBUsSlc",
          "timestamp": 1631630733022
        }
      ]
    },
    "environment": {
      "kernel": "python3",
      "name": "common-cpu.m87",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/base-cpu:m87"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
