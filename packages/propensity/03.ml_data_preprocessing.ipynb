{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxntJHpGGbWJ"
      },
      "outputs": [],
      "source": [
        "# Copyright 2022 Google LLC.\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXzbVhHq1E00"
      },
      "source": [
        "# 3. ML Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afjpo9711ZcN"
      },
      "source": [
        "This notebook demonstrates the preparation of an already created ML dataset for model development. It is vital to split machine learning datasets in such a way that the model performance can be tuned and fairly assessed. This notebook shows an example of dividing a dataset into `out-of-time TEST` dataset (including selected full snapshot/s) and `DEVELOPMENT` dataset (randomly splitting the rest of the snapshots into `TRAIN`,`VALIDATION` and `TEST`). Those names are designed to be directly used in the AUTOML [DATA_SPLIT_COL](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-create-automl#data_split_col)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iL72VhdvwRlN"
      },
      "source": [
        "### Requirements\n",
        "1. Using [ML Windowing Pipeline (MLWP)](https://github.com/google/gps_building_blocks/tree/master/py/gps_building_blocks/ml/data_prep/ml_windowing_pipeline) to create features tables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2K-akTYXwRlN"
      },
      "source": [
        "### Install and import required modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nj1IEIOAs9O"
      },
      "outputs": [],
      "source": [
        "# Uncomment to install required python modules\n",
        "# !sh ../utils/setup.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l8YqU8DxwRlO"
      },
      "outputs": [],
      "source": [
        "# Add custom utils module to Python environment\n",
        "import os\n",
        "import sys\n",
        "sys.path.append(os.path.abspath(os.pardir))\n",
        "\n",
        "import google.auth\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "\n",
        "from gps_building_blocks.cloud.utils import bigquery as bigquery_utils\n",
        "\n",
        "from utils import helpers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFqIBFNtwRlQ"
      },
      "source": [
        "### Notebook custom settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZu8bQGKwRlQ"
      },
      "outputs": [],
      "source": [
        "# Prints all the outputs from cell (instead of using display each time).\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = 'all'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dWnMP1jwRlS"
      },
      "source": [
        "### Set parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "og6PBFQLAUEh"
      },
      "outputs": [],
      "source": [
        "configs = helpers.get_configs('config.yaml')\n",
        "source_configs, dest_configs, run_id_configs = \\\n",
        "    configs.source, configs.destination, configs.run_id\n",
        "\n",
        "# GCP project ID\n",
        "PROJECT_ID = dest_configs.project_id\n",
        "# Name of the BigQuery dataset with MLWP tables.\n",
        "DATASET_NAME = dest_configs.dataset_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ed-O0oJhwRlS"
      },
      "outputs": [],
      "source": [
        "# To distinguish the separate runs of the training pipeline\n",
        "RUN_ID = run_id_configs.train\n",
        "\n",
        "# Initial mwlp tables.\n",
        "FEATURES_TABLE = f'features_{RUN_ID}'\n",
        "\n",
        "# ML datasets\n",
        "# These 4 tables will be created in {DATASET_NAME}\n",
        "FEATURES_SPLIT_TABLE = f'features_split_{RUN_ID}'\n",
        "FEATURES_TEST_TABLE = f'features_test_table_{RUN_ID}'\n",
        "FEATURES_DEV_TABLE = f'features_dev_table_{RUN_ID}'\n",
        "FEATURES_DEV_BALANCED_TABLE = f'features_dev_table_balanced_{RUN_ID}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eIbKMokdwRlP"
      },
      "outputs": [],
      "source": [
        "# Initialize BigQuery Client utils.\n",
        "bq_utils = bigquery_utils.BigQueryUtils(project_id=PROJECT_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-uXsf90wRlS",
        "tags": []
      },
      "source": [
        "### Check feature dataset.\n",
        "\n",
        "1. Determine the right splitting strategy.\n",
        "2. Verify the date to use as a cut-off for the `OUT-OF-TIME TEST` dataset based on the positive rate trends.\n",
        "3. Check imbalance in the dataset and decide on a balancing strategy.\n",
        "4. Consider additional filtering of training data based on snapshot dates.\n",
        "5. Consider selecting a subset of the columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VWQooJIawRlV"
      },
      "outputs": [],
      "source": [
        "# Check list of columns to investigate what features are available,\n",
        "# and potentially selecting a subset of them.\n",
        "sql = f\"\"\"\n",
        "SELECT\n",
        " *\n",
        "FROM\n",
        "  `{DATASET_NAME}.{FEATURES_TABLE}`\n",
        "LIMIT 10;\n",
        "\"\"\"\n",
        "print (sql)\n",
        "df_raw = bq_utils.run_query(sql).to_dataframe()\n",
        "df_raw.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3GcO6qjwRlV"
      },
      "outputs": [],
      "source": [
        "df_raw.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bu-NkJAiwRlW"
      },
      "source": [
        "### Check proportion of positive instances in the features table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9o1aG6X7wRlW"
      },
      "outputs": [],
      "source": [
        "# Produce summary of labels (positive/negative).\n",
        "sql = f\"\"\"\n",
        "SELECT\n",
        "  EXTRACT(DATE FROM snapshot_ts) AS effective_date,\n",
        "  COUNT(*) AS total_records,\n",
        "  COUNTIF(label=TRUE) AS count_positive,\n",
        "  COUNTIF(label=FALSE) AS count_negative,\n",
        "  SAFE_DIVIDE(COUNTIF(label=TRUE),\n",
        "  COUNTIF(label=FALSE)) AS ratio_positive_to_negative\n",
        "FROM\n",
        "  `{DATASET_NAME}.{FEATURES_TABLE}`\n",
        "GROUP BY\n",
        "  1\n",
        "ORDER BY\n",
        "  1 DESC;\n",
        "\"\"\"\n",
        "print (sql)\n",
        "df_raw = bq_utils.run_query(sql).to_dataframe()\n",
        "df_raw.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2nhE4LGwRlW"
      },
      "outputs": [],
      "source": [
        "df_features_check = df_raw.copy(deep=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_sxXK-xBwRlX"
      },
      "outputs": [],
      "source": [
        "fig = px.line(df_features_check,\n",
        "              x='effective_date',\n",
        "              y='ratio_positive_to_negative',\n",
        "              hover_name='count_positive',\n",
        "              title='Positive instances',\n",
        "              height=400)\n",
        "fig.show()\n",
        "df_features_check.head(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1H4Pb0FwRlZ"
      },
      "source": [
        "## Create dataset with split on snapshot dates."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnCfbYsXwRla"
      },
      "source": [
        "#### Get recent effective dates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3cWZ659EwRla"
      },
      "outputs": [],
      "source": [
        "# TODO(michalszczecinski): Change split method to use start_date and end_date.\n",
        "# Get last effective dates.\n",
        "n_last_dates = 3\n",
        "recent_dates = df_features_check['effective_date'].sort_values(\n",
        "    ascending=False).head(n_last_dates).values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nV7rkM6JwRla"
      },
      "outputs": [],
      "source": [
        "# Keep this if you want to use data driven values for last dates in the dataset.\n",
        "test_dates = [str(x) for x in recent_dates]\n",
        "\n",
        "# Define dates here if you want to overwrite with curated dates.\n",
        "# It is useful to keep looking at a date that was used when\n",
        "# evaluating original model so we can make sure all performs as expected.\n",
        "# test_dates = ('2021-05-15', '2021-05-09')\n",
        "if len(test_dates) == 1:\n",
        "  test_dates = f\"('{tuple(test_dates)[0]}')\"\n",
        "else:\n",
        "  test_dates = tuple(test_dates)\n",
        "test_dates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8cgefSttwRla"
      },
      "outputs": [],
      "source": [
        "# Create the dataset if it doesn't exist.\n",
        "# TODO(michalszczecinski): Fix the dataset creation with bq_client(utils version).\n",
        "dataset = bq_utils.client.create_dataset(PROJECT_ID + '.' + DATASET_NAME,\n",
        "                                         exists_ok=True)\n",
        "console_url = 'https://console.cloud.google.com/bigquery?project='\n",
        "print(f'{console_url}{PROJECT_ID}\u0026p={PROJECT_ID}\u0026d={DATASET_NAME}\u0026page=dataset')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3DpzF22wRlb"
      },
      "source": [
        "### Create dataset with columns indicating allocation to TRAIN/VALIDATE/TEST."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1yDeFH3wRlb"
      },
      "outputs": [],
      "source": [
        "# Add additional columns to dataset to indicate which rows are part of train,\n",
        "# validate and test split. This is compliant with automl split conventions.\n",
        "sql = f\"\"\"\n",
        "CREATE OR REPLACE TABLE `{PROJECT_ID}.{DATASET_NAME}.{FEATURES_SPLIT_TABLE}` AS (\n",
        "WITH\n",
        "  ds_features_key AS (\n",
        "  SELECT\n",
        "    *,\n",
        "    FARM_FINGERPRINT(user_id) AS key,\n",
        "  FROM\n",
        "    `{PROJECT_ID}.{DATASET_NAME}.{FEATURES_TABLE}`)\n",
        "SELECT\n",
        "  *,\n",
        "  CASE\n",
        "    WHEN EXTRACT(DATE FROM TIMESTAMP (snapshot_ts)) IN {test_dates}\n",
        "      THEN 'TEST'\n",
        "    WHEN EXTRACT(\n",
        "      DATE FROM TIMESTAMP (snapshot_ts)) NOT IN {test_dates} AND MOD(ABS(key),\n",
        "      10) IN (0,1,2,3,4,5,6,7)\n",
        "      THEN 'TRAIN'\n",
        "    WHEN EXTRACT(\n",
        "      DATE FROM TIMESTAMP (snapshot_ts)) NOT IN {test_dates} AND MOD(ABS(key),\n",
        "      10) IN (8)\n",
        "      THEN 'VALIDATE'\n",
        "    WHEN EXTRACT(\n",
        "      DATE FROM TIMESTAMP (snapshot_ts)) NOT IN {test_dates} AND MOD(ABS(key),\n",
        "      10) IN (9)\n",
        "      THEN 'TEST'\n",
        "  END as data_split,\n",
        "FROM ds_features_key\n",
        ");\n",
        "\"\"\"\n",
        "print (sql)\n",
        "df_raw = bq_utils.run_query(sql).to_dataframe()\n",
        "df_raw.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sSJTG2MwRlb"
      },
      "source": [
        "### Create TEST (OUT-OF-TIME) dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lsyQnADTwRlb"
      },
      "outputs": [],
      "source": [
        "sql = f\"\"\"\n",
        "CREATE OR REPLACE TABLE `{PROJECT_ID}.{DATASET_NAME}.{FEATURES_TEST_TABLE}`\n",
        "AS (\n",
        "  SELECT *\n",
        "  FROM `{PROJECT_ID}.{DATASET_NAME}.{FEATURES_SPLIT_TABLE}`\n",
        "  WHERE EXTRACT(DATE FROM TIMESTAMP (snapshot_ts)) IN {test_dates}\n",
        ");\n",
        "\"\"\"\n",
        "print (sql)\n",
        "df_raw = bq_utils.run_query(sql).to_dataframe()\n",
        "df_raw.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOrZ-S6JVALA"
      },
      "source": [
        "### Create DEVELOPMENT (IN-TIME) dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47Qn4pwOVCMs"
      },
      "outputs": [],
      "source": [
        "sql = f\"\"\"\n",
        "CREATE OR REPLACE TABLE `{PROJECT_ID}.{DATASET_NAME}.{FEATURES_DEV_TABLE}`\n",
        "AS (\n",
        "  SELECT *\n",
        "  FROM `{PROJECT_ID}.{DATASET_NAME}.{FEATURES_SPLIT_TABLE}`\n",
        "  WHERE EXTRACT(DATE FROM TIMESTAMP (snapshot_ts)) NOT IN {test_dates}\n",
        ");\n",
        "\"\"\"\n",
        "print (sql)\n",
        "df_raw = bq_utils.run_query(sql).to_dataframe()\n",
        "df_raw.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYBsqmqIwRlc"
      },
      "source": [
        "## Optional: Create balanced Train dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlZH3gBJwRlc"
      },
      "source": [
        "This optional step could be used to balance the instances when the % of positive examples in a dataset is very small (for example, less than 1%). When balancing, only the `TRAIN` partition of the `TABLE_DS_DEV` is used while keeping `VALIDATE` and `TEST` partitions in their original imbalance ratio reflecting the actual data distribution.\n",
        "\n",
        "Below SQL script keeps all positive instances and then randomly samples records with negative labels; `balance_ratio` is a parameter set to control how many times larger is the number of negative instances comparing to the number of positive instances in the resulting dataset. For example, `balance_ratio=99` would mean that for every positive instance, we are sampling 99 negative instances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vA5Zb93swRlc"
      },
      "outputs": [],
      "source": [
        "# TODO(): Change the way we parametrize the ratio and\n",
        "# test on different imbalance cases.\n",
        "balance_ratio = 99\n",
        "\n",
        "# Following checks imbalance in features by grouping on train, test and validate\n",
        "# datasets. It automatically balances and downsamples dataset to create one for\n",
        "# training.\n",
        "\n",
        "sql = f\"\"\"\n",
        "CREATE OR REPLACE TABLE `{PROJECT_ID}.{DATASET_NAME}.{FEATURES_DEV_BALANCED_TABLE}`\n",
        "AS (\n",
        "  WITH\n",
        "    Dataset AS (\n",
        "      SELECT\n",
        "        *\n",
        "      FROM\n",
        "        `{PROJECT_ID}.{DATASET_NAME}.{FEATURES_SPLIT_TABLE}`\n",
        "      WHERE\n",
        "        data_split = 'TRAIN'\n",
        "    ),\n",
        "    ImbalanceStats AS (\n",
        "      SELECT\n",
        "        data_split,\n",
        "        COUNT(*) AS total_records,\n",
        "        COUNTIF(label = TRUE) AS count_positive,\n",
        "        COUNTIF(label = FALSE) AS count_negative,\n",
        "        FORMAT(\n",
        "          \"%.5f\",\n",
        "          COUNTIF(label = TRUE) / COUNTIF(label = FALSE)\n",
        "        ) AS ratio_positive_to_negative\n",
        "      FROM Dataset\n",
        "      GROUP BY 1\n",
        "      ORDER BY 1\n",
        "    ),\n",
        "    -- Add a random seed to the negative examples.\n",
        "    Negatives AS (\n",
        "      SELECT\n",
        "        *,\n",
        "        RAND() AS rand_seed\n",
        "      FROM\n",
        "        dataset\n",
        "      WHERE\n",
        "        label = FALSE\n",
        "    ),\n",
        "    NegativeSampled AS (\n",
        "      SELECT\n",
        "        * EXCEPT (rand_seed)\n",
        "      FROM\n",
        "        Negatives\n",
        "      WHERE\n",
        "        -- Sampling the negatives using predefined proportion.\n",
        "        rand_seed \u003c= (\n",
        "          SELECT\n",
        "            CAST(ratio_positive_to_negative AS float64)\n",
        "          FROM\n",
        "            ImbalanceStats\n",
        "        ) * {balance_ratio}\n",
        "    )\n",
        "  -- Union all together.\n",
        "  SELECT\n",
        "    *\n",
        "  FROM\n",
        "    NegativeSampled\n",
        "  UNION ALL\n",
        "    -- Add all positive examples.\n",
        "  SELECT\n",
        "    *\n",
        "  FROM\n",
        "    Dataset\n",
        "  WHERE\n",
        "    label = TRUE AND data_split = 'TRAIN'\n",
        ");\n",
        "\"\"\"\n",
        "print (sql)\n",
        "df_raw = bq_utils.run_query(sql).to_dataframe()\n",
        "df_raw.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLBnrxtFwRlc"
      },
      "source": [
        "### Check label balance in the new traning balanced dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pC9fsl4KwRlc"
      },
      "outputs": [],
      "source": [
        "# Produce summary of labels (positive/negative).\n",
        "sql = f\"\"\"\n",
        "SELECT\n",
        "  snapshot_ts,\n",
        "  COUNT(*) AS total_records,\n",
        "  COUNTIF(label = TRUE) AS count_positive,\n",
        "  COUNTIF(label = FALSE) AS count_negative,\n",
        "  FORMAT(\n",
        "    \"%.7f\",\n",
        "    SAFE_DIVIDE(\n",
        "      COUNTIF(label = TRUE),\n",
        "      COUNTIF(label = FALSE)\n",
        "    )\n",
        "  ) AS ratio_positive_to_negative,\n",
        "  FORMAT(\n",
        "    \"%.7f\",\n",
        "    SAFE_DIVIDE(COUNTIF(label = TRUE), COUNT(*))\n",
        "  ) AS ratio_positive\n",
        "FROM\n",
        "  `{DATASET_NAME}.{FEATURES_DEV_BALANCED_TABLE}`\n",
        "GROUP BY 1\n",
        "ORDER BY 1 DESC;\n",
        "\"\"\"\n",
        "\n",
        "print (sql)\n",
        "df_raw = bq_utils.run_query(sql).to_dataframe()\n",
        "df_raw.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aaGTphZaSq8k"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "last_runtime": {
        "build_target": "//corp/gtech/ads/infrastructure/colab_utils/ds_runtime:ds_colab",
        "kind": "private"
      },
      "name": "3.ml_data_preprocessing.ipynb",
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "/piper/depot/google3/third_party/professional_services/solutions/compass/packages/propensity/3_ml_data_preprocessing.ipynb?workspaceId=szczecinski:dev_compass_ml_preprocessing::citc",
          "timestamp": 1630655505596
        },
        {
          "file_id": "/piper/depot/google3/third_party/professional_services/solutions/compass/packages/propensity/3_ml_data_preprocessing.ipynb?workspaceId=szczecinski:dev_compass_ml_preprocessing::citc",
          "timestamp": 1630591931954
        },
        {
          "file_id": "/piper/depot/google3/third_party/professional_services/solutions/compass/packages/propensity/3_ml_data_preprocessing.ipynb?workspaceId=szczecinski:dev_compass_ml_preprocessing::citc",
          "timestamp": 1630425498904
        },
        {
          "file_id": "1bG5YLs51NoiibKl1CPGybSej756rFdkQ",
          "timestamp": 1629087932840
        }
      ]
    },
    "environment": {
      "kernel": "python3",
      "name": "common-cpu.m87",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/base-cpu:m87"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
