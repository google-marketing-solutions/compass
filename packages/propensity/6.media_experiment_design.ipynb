{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DNZzFa33WjvT",
      "metadata": {
        "id": "DNZzFa33WjvT"
      },
      "outputs": [],
      "source": [
        "# Copyright 2021 Google LLC.\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RA_47EEJOY7E",
      "metadata": {
        "id": "RA_47EEJOY7E"
      },
      "source": [
        "# 6. Media experiment design"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ogxuFfmGL27U",
      "metadata": {
        "id": "ogxuFfmGL27U"
      },
      "source": [
        "This notebook demonstrates the design of a media experiment by using the\n",
        "[Experimental Desing](https://github.com/google/gps_building_blocks/tree/master/py/gps_building_blocks/analysis/exp_design)\n",
        "module to activate the predictions from a propensity model. It is vital to design and estimate the impact of media campaigns using valid statistical methods to make sure the limited experimentation budget is utilized effectively and to set the right expectations of the campaign outcome.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "F-53NJikOi25",
      "metadata": {
        "id": "F-53NJikOi25"
      },
      "source": [
        "## Requirements\n",
        "\n",
        "* An already scored test dataset, or the model and the test dataset to be scored available in GCP BigQuery.\n",
        "* This test dataset should contain all the ML instances for at least one snapshot date."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WEr7K1PtL1Xh",
      "metadata": {
        "id": "WEr7K1PtL1Xh"
      },
      "source": [
        "## Install and import required modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BBatmzwvNous",
      "metadata": {
        "id": "BBatmzwvNous"
      },
      "outputs": [],
      "source": [
        "# Install gps_building_blocks package if not installed\n",
        "# !pip install gps_building_blocks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "premium-optimization",
      "metadata": {
        "id": "premium-optimization"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from gps_building_blocks.analysis.exp_design import ab_testing_design\n",
        "from gps_building_blocks.cloud.utils import bigquery as bigquery_utils\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BDX3PHZDUMXo",
      "metadata": {
        "id": "BDX3PHZDUMXo"
      },
      "source": [
        "## Set paramaters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "incorrect-admission",
      "metadata": {
        "id": "incorrect-admission"
      },
      "outputs": [],
      "source": [
        "# GCP Project ID\n",
        "PROJECT_ID = 'project-id'\n",
        "# BigQuery dataset name\n",
        "DATASET = 'dataset'\n",
        "# BigQuery table (name) containing the test dataset to be scored. This test\n",
        "# dataset should contain all the instances at least for one snapshot date\n",
        "TEST_DATA_TABLE = 'test_table'\n",
        "# BigQuery model name\n",
        "MODEL_NAME = 'propensity_model'\n",
        "# BigQuery table (name) containing the test predictions dataset (if available)\n",
        "TEST_DATA_PREDICTIONS_TABLE = 'test_prediction_table'\n",
        "# Selected snapshot date to select the ML instances (reflecting the instances to\n",
        "# be scored on a given scoring date) to be used for experiment design in\n",
        "# YYYY-MM-DD format\n",
        "SELECTED_SNAPSHOT_DATE = '2021-07-01'\n",
        "# Name of the actual label column\n",
        "ACTUAL_LABEL_NAME = 'label'\n",
        "# Name of the prediction column\n",
        "PREDICTED_LABEL_NAME = 'predicted_label'\n",
        "\n",
        "# BigQuery client object\n",
        "bq_client = bigquery_utils.BigQueryUtils(project_id=PROJECT_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XH0C7e7yUsXo",
      "metadata": {
        "id": "XH0C7e7yUsXo"
      },
      "source": [
        "## Score the Test Dataset (if not already scored)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "short-invasion",
      "metadata": {
        "id": "short-invasion"
      },
      "outputs": [],
      "source": [
        "# Prediction sql query\n",
        "prediction_query =\n",
        "(\" SELECT *\n",
        "f\" FROM ML.PREDICT(MODEL `{PROJECT_ID}.{DATASET}.{MODEL_NAME}`, \"\n",
        "f\"                 TABLE `{PROJECT_ID}.{DATASET}.{TEST_DATA_TABLE}`\")\n",
        "\n",
        "# Run prediction\n",
        "test_pred_data = bq_client.query(prediction_query).to_dataframe()\n",
        "\n",
        "# Size of the prediction data frame\n",
        "print(test_pred_data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HO4YCzOCc5yo",
      "metadata": {
        "id": "HO4YCzOCc5yo"
      },
      "source": [
        "## Read the Prediction Test Dataset (if already scored)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "x7FUM0ZAc4bP",
      "metadata": {
        "id": "x7FUM0ZAc4bP"
      },
      "outputs": [],
      "source": [
        "# Data read in sql query\n",
        "read_query = f\"select * from `{PROJECT_ID}.{DATASET}.{TEST_DATA_PREDICTIONS_TABLE}`\"\n",
        "\n",
        "# Run prediction\n",
        "test_pred_data = bq_client.query(prediction_query).to_dataframe()\n",
        "\n",
        "# Size of the prediction data frame\n",
        "print(test_pred_data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rwN9eDGxFzNE",
      "metadata": {
        "id": "rwN9eDGxFzNE"
      },
      "source": [
        "## Select the Relevant Data for Experiment Design"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oriented-elite",
      "metadata": {
        "id": "oriented-elite"
      },
      "outputs": [],
      "source": [
        "# Select all the instances for one snapshot date, which resembles the scoring\n",
        "# dataset for one day. This dataset is used to design the media experiment.\n",
        "selected_snapshot_data = test_pred_data[test_pred_data['snapshot_date'==SELECTED_SNAPSHOT_DATE]]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d-7lf3PPF8UV",
      "metadata": {
        "id": "d-7lf3PPF8UV"
      },
      "source": [
        "## Experiment Design I: Different Propensity Groups\n",
        "\n",
        "One way to use the output from a Propensity Model to optimize marketing is to first define different audience groups based on the predicted probabilities (such as High, Medium and Low propensity groups) and then test the same or different marketing strategies with those. This strategy is more useful to understand how different propensity groups respond to remarketing campaigns.\n",
        "\n",
        "Following step estimates the statistical sample sizes required for different groups (bins) of the predicted probabilities based on different combinations of the expected minimum uplift/effect size, statistical power and statistical confidence levels specified as input parameters.\n",
        "\n",
        "Expected output: a Pandas Dataframe containing statistical sample size for each bin for each combination of minimum uplift_percentage, statistical power and statistical confidence level.\n",
        "\n",
        "Based on the estimated sample sizes and the available sizes one can decide what setting (expected minimum uplift/effect size at a given statistical power and a confidence level) to be selected for the experiment. Then the selected sample sizes could be used to set Test and Control cohorts from each propensity group to implement the media experiment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vHSFXVBFGanG",
      "metadata": {
        "id": "vHSFXVBFGanG"
      },
      "outputs": [],
      "source": [
        "ab_testing_design.calc_chisquared_sample_sizes_for_bins(\n",
        "    labels=selected_snapshot_data[ACTUAL_LABEL_NAME].values)\n",
        "    probability_predictions=selected_snapshot_data[PREDICTED_LABEL_NAME].values),\n",
        "    number_bins=3, # to have High, Medium and Low bins\n",
        "    uplift_percentages=(5, 10, 15), # minimum expected effect sizes\n",
        "    power_percentages=(80, 90),\n",
        "    confidence_level_percentages=(90, 95))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ok5u6DJTHeQi",
      "metadata": {
        "id": "Ok5u6DJTHeQi"
      },
      "source": [
        "## Experiment Design II: Top Propensity Group\n",
        "\n",
        "Another way to use the output from a Propensity Model to optimize marketing is to target the top X% of users having the highest predicted probability in a remarketing campaign or an acquisition campaigns with the similar audience strategy.\n",
        "\n",
        "Following step estimates the statistical sample sizes required for different cumulative groups (bins) of the predicted probabilities (top X%, top 2X% and so on) based on different combinations of the expected minimum uplift/effect size, statistical power and statistical confidence levels specified as input parameters.\n",
        "\n",
        "Expected output: a Pandas Dataframe containing statistical sample size for each cumulative bin for each combination of minimum uplift_percentage, statistical power and statistical confidence level.\n",
        "\n",
        "Based on the estimated sample sizes and the available sizes one can decide what setting (what top X% of users with the expected minimum uplift/effect size at a given statistical power and a confidence level) to be selected for the experiment. Then the selected sample size could be used to set Test and Control cohorts from the top X% to implement the media experiment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sN4gT1OXHVfR",
      "metadata": {
        "id": "sN4gT1OXHVfR"
      },
      "outputs": [],
      "source": [
        "ab_testing_design.calc_chisquared_sample_sizes_for_cumulative_bins(\n",
        "    labels=prediction_df[ACTUAL_LABEL_NAME].values)\n",
        "    probability_predictions=prediction_df[PREDICTED_LABEL_NAME].values),\n",
        "    number_bins=10, # top 10%, 20%, ..., 100%\n",
        "    uplift_percentages=(5, 10, 15), # minimum expected effect sizes\n",
        "    power_percentages=(80, 90),\n",
        "    confidence_level_percentages=(90, 95))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "6.media_experiment_design.ipynb",
      "provenance": []
    },
    "environment": {
      "name": "common-cpu.m65",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/base-cpu:m65"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
