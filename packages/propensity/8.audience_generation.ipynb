{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87cd9c05",
      "metadata": {
        "id": "87cd9c05"
      },
      "outputs": [],
      "source": [
        "# Copyright 2021 Google LLC.\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e563d2a",
      "metadata": {
        "id": "2e563d2a"
      },
      "source": [
        "This notebook demonstrates the generation of propensity audience. It exports the audience segmented into Test and Control to the BigQuery table. This data then can be uploaded via measurement protocol to GA and used for the activation with the Google Ads products.\n",
        "\n",
        "**Requirements:**\n",
        "* An already scored dataset.\n",
        "* This is the prediction dataset, it contains ML instances based on the `user_id` and `snapshot_ts`, from which we can select the audience to target."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89b380f5",
      "metadata": {
        "id": "89b380f5"
      },
      "source": [
        "## Install and import required modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5c220df",
      "metadata": {
        "id": "f5c220df"
      },
      "outputs": [],
      "source": [
        "# Uncomment to install required python modules\n",
        "# !pip install -r requirements.txt -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2f61cbb",
      "metadata": {
        "id": "a2f61cbb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from gps_building_blocks.cloud.utils import bigquery as bigquery_utils"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51254ad9",
      "metadata": {
        "id": "51254ad9"
      },
      "source": [
        "## Set paramaters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89618c5c",
      "metadata": {
        "id": "89618c5c"
      },
      "outputs": [],
      "source": [
        "# GCP Project ID\n",
        "PROJECT_ID = 'project'\n",
        "# Name of BigQuery dataset,\n",
        "# destination for created tables for modelling and activation.\n",
        "DATASET = 'dataset'\n",
        "# BigQuery table name containing the predictions\n",
        "PREDICTIONS_TABLE = 'test_prediction_table'\n",
        "# Selected snapshot date to select the ML instances\n",
        "# YYYY-MM-DD format\n",
        "SELECTED_SNAPSHOT_DATE = '2021-05-31'\n",
        "# Name of the column in the prediction table with the predicted label\n",
        "PREDICTED_LABEL_NAME = 'predicted_label_probs'\n",
        "# Label value for the positive class.\n",
        "POSITIVE_CLASS_LABEL = True\n",
        "# Name of the table with exported audience.\n",
        "AUDIENCE_EXPORT_TABLE = 'audience_export'\n",
        "# Original GA dataset for joining client_id and fullvisitor_id.\n",
        "# e.x. bigquery-public-data.google_analytics_sample.ga_sessions_* for\n",
        "# Google Merchandize Store GA360 dataset\n",
        "SOURCE_DATA = 'project.dataset.ga_sessions_*'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4180295a",
      "metadata": {
        "id": "4180295a"
      },
      "outputs": [],
      "source": [
        "bq_utils = bigquery_utils.BigQueryUtils(project_id=PROJECT_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8c96833",
      "metadata": {
        "id": "e8c96833"
      },
      "source": [
        "## Read the Prediction Test Dataset (if already scored)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65e13f5c",
      "metadata": {
        "id": "65e13f5c"
      },
      "source": [
        "In this step, we assume the prediction dataset is available as a BigQuery table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f802c4ea",
      "metadata": {
        "id": "f802c4ea"
      },
      "outputs": [],
      "source": [
        "# SQL for extracting prediction dataset when using BQML.\n",
        "sql = f\"\"\"\n",
        "  SELECT\n",
        "    user_id,\n",
        "    snapshot_ts,\n",
        "    days_since_latest_activity,\n",
        "    days_since_first_activity,\n",
        "    probs.label AS predicted_score_label,\n",
        "    probs.prob AS score\n",
        "  FROM\n",
        "    `{PROJECT_ID}.{DATASET}.{PREDICTIONS_TABLE}` AS predictions,\n",
        "    UNNEST({PREDICTED_LABEL_NAME}) AS probs\n",
        "  WHERE\n",
        "    probs.label={POSITIVE_CLASS_LABEL}\n",
        "    AND snapshot_ts='{SELECTED_SNAPSHOT_DATE}';\n",
        "\"\"\"\n",
        "print (sql)\n",
        "df_prediction = bq_utils.run_query(sql).to_dataframe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8f61dda",
      "metadata": {
        "id": "a8f61dda"
      },
      "outputs": [],
      "source": [
        "df_prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87be2f9c",
      "metadata": {
        "id": "87be2f9c"
      },
      "source": [
        "### Add quantiles to dataframe with user_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "984f80de",
      "metadata": {
        "id": "984f80de"
      },
      "outputs": [],
      "source": [
        "# Set the number of quantiles that you want to divide users into.\n",
        "number_bins = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "434dfa44",
      "metadata": {
        "id": "434dfa44"
      },
      "outputs": [],
      "source": [
        "df_prediction['bins_10'] = pd.cut(\n",
        "    df_prediction['score'] * 100,\n",
        "    bins=[0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100])\n",
        "df_prediction['bins_100'] = pd.cut(\n",
        "    df_prediction['score'] * 100, bins=np.arange(0, 101, 1))\n",
        "df_prediction['score_quantile'] = pd.qcut(\n",
        "    df_prediction['score'], q=number_bins, labels=False)\n",
        "df_prediction['score_quantile_labels'] = pd.qcut(\n",
        "    df_prediction['score'], q=number_bins)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "491365d4",
      "metadata": {
        "id": "491365d4"
      },
      "outputs": [],
      "source": [
        "df_prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17ec9b77",
      "metadata": {
        "id": "17ec9b77"
      },
      "source": [
        "## Check scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d1dad99",
      "metadata": {
        "id": "7d1dad99"
      },
      "outputs": [],
      "source": [
        "# Check score distribution.\n",
        "df_prediction.score.hist(bins=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fd97ef5",
      "metadata": {
        "id": "5fd97ef5"
      },
      "outputs": [],
      "source": [
        "df_prediction['score'].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5af747a0",
      "metadata": {
        "id": "5af747a0"
      },
      "outputs": [],
      "source": [
        "# Check ranges of the scores by quantile.\n",
        "segments =['score_quantile']\n",
        "df_summary = df_prediction.groupby(segments)[['user_id']].count()\n",
        "df_summary['score_quantile_labels'] = df_prediction.groupby(\n",
        "    segments)[['score_quantile_labels']].min()\n",
        "df_summary['score_min'] = df_prediction.groupby(segments)[['score']].min()\n",
        "df_summary['score_max'] = df_prediction.groupby(segments)[['score']].max()\n",
        "df_summary['score_mean'] = df_prediction.groupby(segments)[['score']].mean()\n",
        "df_summary['records_pct'] = (df_summary['user_id'] /\n",
        "                             df_summary['user_id'].sum()) * 100\n",
        "\n",
        "df_summary"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "392948ff",
      "metadata": {
        "id": "392948ff"
      },
      "source": [
        "## Exporting data audience to the table"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8269f38",
      "metadata": {
        "id": "a8269f38"
      },
      "source": [
        "### Creating control and test groups\n",
        "\n",
        "\n",
        "This is a very simple illustrative method of splitting users into Test and Control groups.\n",
        "If you are going to do multiple refresh uploads then it is strongly adviced to use another method based on the unique fingerfrinting of users and tracking their participation in the Control/Test groups. We can also  score and upload only new users that have not been assigned before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1528a31",
      "metadata": {
        "id": "e1528a31"
      },
      "outputs": [],
      "source": [
        "n_control = 10000\n",
        "control_ids = df_prediction['user_id'].sample(n_control).values\n",
        "control_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46c78180",
      "metadata": {
        "id": "46c78180"
      },
      "outputs": [],
      "source": [
        "# All users in randomised control_ids will be assigned to Control (True),\n",
        "# the rest of the users is going to be assigned to Test (False).\n",
        "df_prediction['control'] = df_prediction['user_id'].isin(control_ids)\n",
        "df_prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d4c6a45",
      "metadata": {
        "id": "4d4c6a45"
      },
      "outputs": [],
      "source": [
        "# Set quantile threshold to select for upload.\n",
        "quantile_threshold = 9\n",
        "\n",
        "# For the exported table we are selecting all users in control\n",
        "# and also all users above quantile threshold.\n",
        "# This is because part of those users have been already assigned to the control,\n",
        "# and rest is in the Test group.\n",
        "mask = (\n",
        "    (df_prediction['score_quantile'] \u003e= quantile_threshold) |\n",
        "    (df_prediction['control'] == True))\n",
        "df_selected = df_prediction[mask]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "852b40fc",
      "metadata": {
        "id": "852b40fc"
      },
      "outputs": [],
      "source": [
        "df_selected"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "940b776d",
      "metadata": {
        "id": "940b776d"
      },
      "source": [
        "### Checking stats by control-test split\n",
        "\n",
        "By looking at the mean score difference between Test and Control groups we can get an idea about the performance we can expect and select the groups accordingly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "627f11e4",
      "metadata": {
        "id": "627f11e4"
      },
      "outputs": [],
      "source": [
        "segments =['control']\n",
        "df_summary = df_selected.groupby(segments)[['user_id']].count()\n",
        "df_summary['score_min'] = df_selected.groupby(segments)[['score']].min()\n",
        "df_summary['score_max'] = df_selected.groupby(segments)[['score']].max()\n",
        "df_summary['score_mean'] = df_selected.groupby(segments)[['score']].mean()\n",
        "df_summary['score_median'] = df_selected.groupby(segments)[['score']].median()\n",
        "df_summary['records_pct'] = (\n",
        "    df_summary['user_id'] / df_summary['user_id'].sum()) * 100\n",
        "df_summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33e8ef47",
      "metadata": {
        "id": "33e8ef47"
      },
      "outputs": [],
      "source": [
        "# How many times better are the average scores in the Test group than Control.\n",
        "df_summary['score_mean'][False] / df_summary['score_mean'][True]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26fece50",
      "metadata": {
        "id": "26fece50"
      },
      "source": [
        "### Export data for experiment to the BigQuery table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c325425",
      "metadata": {
        "id": "0c325425"
      },
      "outputs": [],
      "source": [
        "cols = ['user_id',\n",
        "        'snapshot_ts',\n",
        "        'score',\n",
        "        'days_since_first_activity',\n",
        "        'days_since_latest_activity',\n",
        "        'score_quantile',\n",
        "        'control']\n",
        "df_selected[cols]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b32180a",
      "metadata": {
        "id": "4b32180a"
      },
      "outputs": [],
      "source": [
        "destination_table = f'{DATASET}.{AUDIENCE_EXPORT_TABLE}'\n",
        "destination_table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d0fbdd4",
      "metadata": {
        "id": "6d0fbdd4"
      },
      "outputs": [],
      "source": [
        "df_selected[cols].to_gbq(\n",
        "    destination_table=destination_table,\n",
        "    project_id=PROJECT_ID,\n",
        "    if_exists='replace')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "8.audience_generation.ipynb",
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "/piper/depot/google3/third_party/professional_services/solutions/compass/packages/propensity/8a.audience_generation.ipynb?workspaceId=szczecinski:dev_audience_generation::citc",
          "timestamp": 1632737106105
        },
        {
          "file_id": "/piper/depot/google3/third_party/professional_services/solutions/compass/packages/propensity/8a.audience_generation.ipynb?workspaceId=szczecinski:dev_audience_generation::citc",
          "timestamp": 1631631928007
        },
        {
          "file_id": "1Dh3leR4-p532CVEYY613FKWH9bBUsSlc",
          "timestamp": 1631630733022
        }
      ]
    },
    "environment": {
      "name": "common-cpu.m79",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/base-cpu:m79"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
