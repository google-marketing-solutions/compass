{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e290d603",
      "metadata": {
        "id": "e290d603"
      },
      "outputs": [],
      "source": [
        "# Copyright 2021 Google LLC.\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bae240f",
      "metadata": {
        "id": "6bae240f"
      },
      "source": [
        "# 5. Model Evaluation and Diagnostics"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a93826b",
      "metadata": {
        "id": "9a93826b"
      },
      "source": [
        "This notebook demonstrates the evaluation of a propensity model by using the\n",
        "[Binary Classication Diagnostics](https://github.com/google/gps_building_blocks/blob/master/py/gps_building_blocks/ml/diagnostics/binary_classification.py)\n",
        "module.\n",
        "\n",
        "This evaluation consists of:\n",
        "* Model performance with respect to a variety of metrics.\n",
        "* Plots to understand the model performance in relation to the different propensity groups helping to design media experiments.\n",
        "* Model insights (the relationship between features and the predictions/label) helping to generate new business insights.\n",
        "* Insights helping to diagnose the model to make sure it is reasonable (e.g. no label leakage in features)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31333fe2",
      "metadata": {
        "id": "31333fe2"
      },
      "source": [
        "### Requirements\n",
        "* The model and the Testing dataset should be available in GCP BigQuery."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1aec16f3",
      "metadata": {
        "id": "1aec16f3"
      },
      "source": [
        "## Install and import required modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2212e2ae",
      "metadata": {
        "id": "2212e2ae"
      },
      "outputs": [],
      "source": [
        "# Install gps_building_blocks package if not installed.\n",
        "# !pip install gps_building_blocks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51b4ca36",
      "metadata": {
        "id": "51b4ca36"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from gps_building_blocks.cloud.utils import bigquery as bigquery_utils\n",
        "from gps_building_blocks.ml.diagnostics import binary_classification"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17cb7cf1",
      "metadata": {
        "id": "17cb7cf1"
      },
      "source": [
        "## Set paramaters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf5d9613",
      "metadata": {
        "id": "bf5d9613"
      },
      "outputs": [],
      "source": [
        "# GCP Project ID\n",
        "PROJECT_ID = 'project-id'\n",
        "# BigQuery dataset name\n",
        "DATASET = 'dataset'\n",
        "# BigQuery table name with the test dataset ready for scoring.\n",
        "TEST_DATA_TABLE = 'test_table'\n",
        "# BigQuery model name\n",
        "MODEL_NAME = 'propensity_model'\n",
        "# BigQuery table name containing the scored test dataset.\n",
        "TEST_DATA_PREDICTIONS_TABLE = 'test_prediction_table'\n",
        "# Optional: selected snapshot date to filter ML instances,\n",
        "# reflecting the instances to be evaluated on a given scoring date\n",
        "# YYYY-MM-DD format\n",
        "SELECTED_SNAPSHOT_DATE = '2021-05-31'\n",
        "# Name of the column in the prediction table with the predicted label\n",
        "PREDICTED_LABEL_NAME = 'predicted_label_probs'\n",
        "# Name of the column in the prediction table with the actual label\n",
        "ACTUAL_LABEL_NAME = 'label'\n",
        "# Label value for the positive class\n",
        "POSITIVE_CLASS_LABEL = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01dce4e5",
      "metadata": {
        "id": "01dce4e5"
      },
      "outputs": [],
      "source": [
        "bq_client = bigquery_utils.BigQueryUtils(project_id=PROJECT_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a09646a",
      "metadata": {
        "id": "5a09646a"
      },
      "source": [
        "## Run Model Diagnostics"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47b6e6c7",
      "metadata": {
        "id": "47b6e6c7"
      },
      "source": [
        "## Create Scored Test Dataset (if not already scored)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5951662",
      "metadata": {
        "id": "a5951662"
      },
      "outputs": [],
      "source": [
        "# Prediction sql query\n",
        "# TODO() replace filtering with list of dates\n",
        "prediction_query =f\"\"\"\n",
        "  CREATE OR REPLACE TABLE `{PROJECT_ID}.{DATASET}.{TEST_DATA_PREDICTIONS_TABLE}` AS (\n",
        "  SELECT *\n",
        "  FROM ML.PREDICT(MODEL `{PROJECT_ID}.{DATASET}.{MODEL_NAME}`,\n",
        "                  TABLE `{PROJECT_ID}.{DATASET}.{TEST_DATA_TABLE}`)\n",
        "  -- Uncomment line below if you want to filter instances on the snapshot date\n",
        "  -- WHERE snapshot_ts='{SELECTED_SNAPSHOT_DATE}')\n",
        "\"\"\"\n",
        "print(prediction_query)\n",
        "bq_client.run_query(prediction_query)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce5e42d5",
      "metadata": {
        "id": "ce5e42d5"
      },
      "source": [
        "## Read the Predition Test Dataset (after scoring, in BQML format)\n",
        "\n",
        "In this step, we assume the prediction dataset is available as a BQ table, for example produced by AUTOML model using batch scoring. Following functions require a Pandas DataFrame with columns containing the binary label (1.0 and 0.0 values) and predicted probabilities (between 0.0 and 1.0) so we are going to transform loaded data into dataframe with this format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3451a21",
      "metadata": {
        "id": "e3451a21"
      },
      "outputs": [],
      "source": [
        "# Sql for extracting prediction dataset when using BQML.\n",
        "sql = f\"\"\"\n",
        "SELECT\n",
        "  predictions.label AS label,\n",
        "  predicted_label,\n",
        "  probs.label AS predicted_score_label,\n",
        "  probs.prob AS score,\n",
        "  *\n",
        "  #remove duplicates of columns like label_1\n",
        "  EXCEPT(predicted_label_probs,\n",
        "    label,\n",
        "    predicted_label,\n",
        "    prob)\n",
        "FROM\n",
        "  `{PROJECT_ID}.{DATASET}.{TEST_DATA_PREDICTIONS_TABLE}` AS predictions,\n",
        "  UNNEST({PREDICTED_LABEL_NAME}) AS probs\n",
        "WHERE\n",
        "  probs.label={POSITIVE_CLASS_LABEL};\n",
        "\"\"\"\n",
        "print (sql)\n",
        "df_prediction = (\n",
        "    bq_client.run_query(sql).to_dataframe())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e693692",
      "metadata": {
        "id": "0e693692"
      },
      "source": [
        "### Check loaded outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f2ee441",
      "metadata": {
        "id": "3f2ee441"
      },
      "outputs": [],
      "source": [
        "df_prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e98876dd",
      "metadata": {
        "id": "e98876dd"
      },
      "outputs": [],
      "source": [
        "df_prediction[ACTUAL_LABEL_NAME].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f35dee5",
      "metadata": {
        "id": "4f35dee5"
      },
      "outputs": [],
      "source": [
        "# Extract score for predicting positive label.\n",
        "# Change positive label into 1.0.\n",
        "df_prediction['label_numerical'] = [1.0 if label==POSITIVE_CLASS_LABEL else 0.0\n",
        "                          for label in df_prediction[ACTUAL_LABEL_NAME]]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4913150",
      "metadata": {
        "id": "c4913150"
      },
      "source": [
        "### Check transformed outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08df491a",
      "metadata": {
        "id": "08df491a"
      },
      "outputs": [],
      "source": [
        "df_prediction.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fed9e87",
      "metadata": {
        "id": "7fed9e87"
      },
      "outputs": [],
      "source": [
        "df_prediction['label_numerical'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31329453",
      "metadata": {
        "id": "31329453"
      },
      "source": [
        "## Performance metrics\n",
        "\n",
        "The following function calculates following metrics:\n",
        "\n",
        "* prop_positives: Proportion of positive instances in the dataset scored\n",
        "* auc_roc: Area under the ROC curve (more on this below).\n",
        "* auc_pr: Area under the Precision Recall Curve (more on this below).\n",
        "* binarize_threshold: The probability threshold used to binarize the predictions to calculate the following performance metrics.\n",
        "* accuracy: overall accuracy of the predictions.\n",
        "* true_positive_rate: (Recall or Sensitivity) proportion of positive. instances correctly predicted out of all the positive instances in the dataset.\n",
        "* true_negative_rate: (Specificity) proportion of negative instances. correctly predicted out of all the negative instances in the dataset.\n",
        "* precision: (Confidence) proportion of positive instances correctly predicted out of all the instances predicted as positives.\n",
        "* f1_score: A weighted average between Precision and Recall.\n",
        "\n",
        "We need to provide:\n",
        "\n",
        "`label` -  an array of true binary labels represented in merical form (1.0 and 0.0)\n",
        "`probability predictions` - an array of predicted probabilities between 0.0 and 1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b1d7182",
      "metadata": {
        "id": "7b1d7182"
      },
      "outputs": [],
      "source": [
        "binary_classification.calc_performance_metrics(\n",
        "    labels = df_prediction['label_numerical'].values,\n",
        "    probability_predictions = df_prediction['score'].values)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1dbf1842",
      "metadata": {
        "id": "1dbf1842"
      },
      "source": [
        "## ROC Curve (True Positive Rate vs False Positive Rate)\n",
        "\n",
        "The following function plots the receiver operating characteristic (ROC) curve, which illustrates the relationship between true positive rate and false positive rate of the predictions from a binary classifier system. As the major metric to evaluate the ROC curve, the AUC (Area Under The Curve) is also printed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbf1d25e",
      "metadata": {
        "id": "bbf1d25e"
      },
      "outputs": [],
      "source": [
        "binary_classification.plot_roc_curve(\n",
        "    labels=df_prediction['label_numerical'].values,\n",
        "    probability_predictions=df_prediction['score'].values)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe91cb4f",
      "metadata": {
        "id": "fe91cb4f"
      },
      "source": [
        "## Precision-Recall Curve\n",
        "\n",
        "The following function plots the precision-recall curve, which illustrates the relationship between the precision and recall of the predictions from a binary classifier system. In general a high precision rate relates to a low false positive rate, while a high recall rate relates to a low false negative rate. By showing the tradeoff between precision and recall with different thresholds on the predictions, we are able to find out the optimal threshold for the specific problem. Moreover, the Average Precision (AP), which calculates the area under the precision-recall curve, is also printed for the evaluation of precision recall curve."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b82fbe5c",
      "metadata": {
        "id": "b82fbe5c"
      },
      "outputs": [],
      "source": [
        "binary_classification.plot_precision_recall_curve(\n",
        "    labels=df_prediction['label_numerical'].values,\n",
        "    probability_predictions=df_prediction['score'].values)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "538d41e9",
      "metadata": {
        "id": "538d41e9"
      },
      "source": [
        "## Predicted Probability Distributions\n",
        "\n",
        "The following function plots the distributions of predicted probabilities for each class. This illustrates how distinguishable the predicted probabilities for different classes are."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a478d2bf",
      "metadata": {
        "id": "a478d2bf"
      },
      "outputs": [],
      "source": [
        "binary_classification.plot_predicted_probabilities(\n",
        "    labels=df_prediction['label_numerical'].values,\n",
        "    probability_predictions=df_prediction['score'].values)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7083c98e",
      "metadata": {
        "id": "7083c98e"
      },
      "source": [
        "## Calculate and plot performance metrics for Bins of the Probabilities\n",
        "\n",
        "The following function does following:\n",
        "\n",
        "1. Sorts predicted probabilities in the descending order, then divides the instances into N number of equal sized bins (e.g. deciles when N = 10) such that the first bin has the instances with the highest probabilities and so on.\n",
        "2. Calculates the Precision, Precision Uplift (Precision of the bin divided by the proportion of positive instances in the dataset indicating the Precision of selecting a random sample of the size of the bin) and Coverage (or Recall = the proportion of positive instances in the bin out of all the positive instances in the dataset) for each bin.\n",
        "\n",
        "**Observations:** These plots can give a good understanding of the model performance for these different bins of the probability predictions. Generally we would expect to see a monotonically decreasing trend of these metrics going from the top bin to the bottom bin. If we see some different pattern from this plot, it would create some doubts of the quality of the model leading to further investigations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8678007",
      "metadata": {
        "id": "a8678007"
      },
      "outputs": [],
      "source": [
        "bin_metrics = binary_classification.calc_bin_metrics(\n",
        "    labels=df_prediction['label_numerical'].values,\n",
        "    probability_predictions=df_prediction['score'].values,\n",
        "    number_bins=10,\n",
        "    decimal_points=4)\n",
        "\n",
        "binary_classification.plot_bin_metrics(bin_metrics=bin_metrics, )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30b73d0f",
      "metadata": {
        "id": "30b73d0f"
      },
      "outputs": [],
      "source": [
        "bin_metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "833746d8",
      "metadata": {
        "id": "833746d8"
      },
      "source": [
        "## Calculate and plot performance metrics for Cumulative Bins of the Probabilities\n",
        "\n",
        "The following function plots does following:\n",
        "\n",
        "1. Sorts predicted probabilities in the descending order, and then divides the instances into N number of bins with increasing size. For example, when N = 10, it creates 10 bins such that the first bin contains the top 10% instances with the highest probability, the second bin contains the top 20% instances with the highest probability and so on where the last bin contains all the instances.\n",
        "2. Calculates the Precision, Precision Uplift (Precision of the bin divided by the proportion of positive instances in the dataset indicating the Precision of selecting a random sample of the size of the bin) and Coverage (or Recall = the proportion of positive instances in the bin out of all the positive instances in the dataset) for each bin.\n",
        "\n",
        "**Observations:** From these plots we would generally expect the Precision and Precision uplift to monotonically decrease and Recall to monotonically increase when we go from the high probability bins to the lower ones. These plots give us a good understanding of the expected precision and recall values for example if we select top N% of the instances.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5451547",
      "metadata": {
        "id": "e5451547"
      },
      "outputs": [],
      "source": [
        "cumulative_bin_metrics = binary_classification.calc_cumulative_bin_metrics(\n",
        "    labels=df_prediction['label_numerical'].values,\n",
        "    probability_predictions=df_prediction['score'].values,\n",
        "    number_bins=10,\n",
        "    decimal_points=4)\n",
        "\n",
        "binary_classification.plot_cumulative_bin_metrics(\n",
        "    cumulative_bin_metrics=cumulative_bin_metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97d76de6",
      "metadata": {
        "id": "97d76de6"
      },
      "source": [
        "## Feature Exploration Plots\n",
        "\n",
        "This section calculates and plots the distribution of features in the dataset for the equal sized bins of the predicted probability.\n",
        "\n",
        "How it works:\n",
        "1. First sorts predicted probabilities in the descending order, and then divides the instances into N number of equal sized bins (e.g. deciles when N = 10) such that the first bin has the instances with the highest probabilities and so on.\n",
        "2. Then it calculates and plots the distribution of each feature for each plot.\n",
        "\n",
        "**Expected outputs:**\n",
        "\n",
        "The output plots can be used to understand the relationships (positive, negative and non-linear correlations) between the features and the predictions (labels) leading to:\n",
        "\n",
        "* extracting new insights in relation to the business problem (e.g. how the demographics, user-behaviour or functionalities in the app/website related to the conversion rate).\n",
        "* confirming the relationships the model has learned between features and label makes sense (e.g. it hasn't learned any suspicious relationships caused by label leakages due to data collection or processing issues).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7632467a",
      "metadata": {
        "id": "7632467a"
      },
      "source": [
        "### Read in the table prediction table schema."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52f19479",
      "metadata": {
        "id": "52f19479"
      },
      "outputs": [],
      "source": [
        "sql = f\"\"\"SELECT column_name, data_type\n",
        "       FROM `{PROJECT_ID}.{DATASET}`.INFORMATION_SCHEMA.COLUMNS\n",
        "       WHERE table_name='{TEST_DATA_PREDICTIONS_TABLE}'\"\"\"\n",
        "\n",
        "print(sql)\n",
        "schema = bq_client.run_query(sql).to_dataframe()\n",
        "schema.columns = ['column_name', 'type']\n",
        "schema.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb01a81a",
      "metadata": {
        "id": "fb01a81a"
      },
      "outputs": [],
      "source": [
        "# Seperate out numerical and categorical columns names.\n",
        "num_features = list(schema[schema['type'].isin(['INT64','FLOAT64'])]['column_name'])\n",
        "cat_features = list(schema[schema['type'].isin(['STRING'])]['column_name'])\n",
        "# Remove non-feature columns.\n",
        "columns_to_remove = ['user_id', 'label']\n",
        "num_features = [v for v in num_features if v not in columns_to_remove]\n",
        "cat_features = [v for v in cat_features if v not in columns_to_remove]\n",
        "print(f'Number of numerical features: {len(num_features)}')\n",
        "print(f'Number of categorical features: {len(cat_features)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb948aa7",
      "metadata": {
        "id": "eb948aa7"
      },
      "outputs": [],
      "source": [
        "feature_names_to_plot = num_features + cat_features\n",
        "feature_types = ['numerical'] * len(num_features) + ['categorical'] * len(cat_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14becc7c",
      "metadata": {
        "id": "14becc7c"
      },
      "outputs": [],
      "source": [
        "binary_classification.plot_binned_features(\n",
        "    data=df_prediction,\n",
        "    prediction_column_name='score',\n",
        "    feature_names=feature_names_to_plot,\n",
        "    feature_types=feature_types);"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "5.model_evaluation_and_diagnostics.ipynb",
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "/piper/depot/google3/third_party/professional_services/solutions/compass/packages/propensity/5.model_evaluation_and_diagnostics.ipynb?workspaceId=szczecinski:dev_model_diagnostics::citc",
          "timestamp": 1631805049021
        },
        {
          "file_id": "1MgZHVijNLCNIXbwU-zKPrXHl4sJZ8RHT",
          "timestamp": 1631262181717
        },
        {
          "file_id": "/piper/depot/google3/third_party/professional_services/solutions/compass/packages/propensity/5_model_evaluation_and_diagnostics.ipynb?workspaceId=szczecinski:diagnostic_prod::citc",
          "timestamp": 1626444813368
        },
        {
          "file_id": "/piper/depot/google3/third_party/professional_services/solutions/compass/packages/propensity/5_model_evaluation_and_diagnostics.ipynb?workspaceId=szczecinski:diagnostic_prod::citc",
          "timestamp": 1626442594736
        },
        {
          "file_id": "1dugCt0TXinm1NqCEUiuFfyxWS54mDekp",
          "timestamp": 1625469937832
        }
      ]
    },
    "environment": {
      "name": "common-cpu.m78",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/base-cpu:m78"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
